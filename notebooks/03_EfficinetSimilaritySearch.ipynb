{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Similarity Search    \n",
    "\n",
    "## CSCI E-108      \n",
    "\n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these exercises you will gain some experience with methods for efficient similarity search. A naive similarity search requires a brute-force computation of all pairwise distances, in the chosen metrics. The brute-force approach is not scalable with computational complexity for $n$ observations is $O(n^2)$. For massive dataset we need much more efficient methods. Here, we will explore two possibilities:        \n",
    "1. [**KD trees**](https://en.wikipedia.org/wiki/K-d_tree) are constructed from recurrsive binary splits. For a dataset with $n$ observatrions and dimensionality, $d$, the computational complexity is $O\\big(d\\ n \\log(n) \\big)$. While not ideal linear scaling, $O(n)$,the KD tree is a significant improvement over brute-force methods. Generally, KD-trees are considered to be efficient for large datasets with $d \\le 20$. For higher dimensional data, R-trees and ball trees can be extended to higher dimensional data. We will apply ball trees for clustering and dimensionality reduction with the spectral clustering and UMAP algorithms.     \n",
    "2. [**Mini-hashing**](https://en.wikipedia.org/wiki/MinHash) and [**locally sensitive hashing (LSH)**](https://en.wikipedia.org/wiki/Locality-sensitive_hashing#:~:text=In%20computer%20science%2C%20locality%2Dsensitive,universe%20of%20possible%20input%20items.)) can be applied to massive very high dimensional dataset. The mini-hash algorithm creates sketches of the observations that approximate the Jacard similarity between two varaibles. Multiple bands of sketches are combined to create locally sensive hashes with lower false positive and false negative rates.     \n",
    "\n",
    "Before proceeding to the exercises, execute the code in the cell below to import the required packages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search with KD Trees     \n",
    "\n",
    "We will now explore a basic example of using a KD tree to find nearest neighbors. We will use a small (toy) dataset of characteristics and metabolic measurements taken from 442 diabetes patients. The KD tree algorithm will be used to find the nearest neightbors of the patients, based on the Euclidean distrance between the observation vectors.    \n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "The diabetes dataset is in the [Scikit Learn Datasets package](https://scikit-learn.org/stable/api/sklearn.datasets.html).This dataset has already been clearned, missing values dealt with, and standardized (zero mean, unite variance). Therefore, we will skip the usual exploration and preparation steps.     \n",
    "\n",
    "Execute the code in the cell below to load the dataset and split it into two data frames, one for training and one for test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 20\n",
    "column_names = ['age_years',\n",
    "                'sex',\n",
    "                'BMI',\n",
    "                'Average_BP',\n",
    "                'serum_cholesterol',\n",
    "                'LDL',\n",
    "                'HDL',\n",
    "                'total_chol_over_HDL',\n",
    "                'log_serum_triglycerides',\n",
    "                'Glucose_level']\n",
    "\n",
    "diabetes_test, diabetes_train = train_test_split(datasets.load_diabetes()['data'], train_size=train_size)\n",
    "\n",
    "diabetes_train = pd.DataFrame(diabetes_train, columns=column_names)\n",
    "diabetes_test = pd.DataFrame(diabetes_test, columns=column_names)\n",
    "print('Dimensions of training data frame = ' + str(diabetes_train.shape))\n",
    "print(diabetes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each of the observations is a numeric vector with 10 variables. These variables have all be standardized.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing and querying the KD-Tree   \n",
    "\n",
    "The code in the cell below constructs the KD-tree as a KD-tree object with the required arguments. Execute the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time KD_tree = KDTree(diabetes_train, leaf_size=10, metric='euclidean') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how quickly the tree was constructed. This should not be surprising given the small data set and low dimensionality of the data.   \n",
    "\n",
    "With the tree constructed we can now query the tree with the test data. The number of near neighbors is specified as 1 so that the query returns the single nearest neighbors.    \n",
    "\n",
    "> **Note:** In this case we are interested in similarity with each of the observations used to construct the KD-tree, so we use $k=1$ nearest neighbors. For other purposes, such as constructing a nearest neighbor graph, a larger value of $k$ is used. We will encounter a number algorithms using nearest neighbor graphs in subsequent lessons.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4545)\n",
    "%time distances, neighbors = KD_tree.query(diabetes_test, k=1)\n",
    "distance_frame = pd.DataFrame({'Nearest Neighbor':neighbors.ravel(), 'Distance':distances.ravel()})\n",
    "distance_frame.sort_values('Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next print some summary statistics for the 20 queries just performed on the KD-tree by executing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stats for the tree\")\n",
    "print(\"Number of trims = %5d \\nnumber of leaves = %5d \\nnumber of splits = %6d\" % KD_tree.get_tree_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can filter by similarities to find the nodes of the KD-tree that are similar to the new observations in the test dataset by applying a threshold to the distance measure. For the purpose of demonstration, we select an arbitrary distance threshold to filter on. Execute the code in the cell below to apply a similarity threshold and display the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisor = 2.5\n",
    "distance_threshold = round(np.max(distance_frame.Distance)/divisor, 4)\n",
    "print('With divisor = ' + str(divisor) + ' and distance threshold = ' + str(distance_threshold))\n",
    "mask = distances < distance_threshold\n",
    "\n",
    "n_samples = len(diabetes_test)\n",
    "similarity_results = pd.DataFrame({'Sample':range(n_samples), 'Neighbor':neighbors.ravel(), 'Distance':distance_frame.Distance}).loc[mask.ravel(),:]\n",
    "similarity_results.sort_values('Distance', inplace=True)\n",
    "\n",
    "print('\\nThe total number of similar cases = ' + str(sum(mask)[0]))\n",
    "print(similarity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-1:** Examine the results of the similarity search using the KD-tree algorithm and answer the following questions:\n",
    "> 1. Given the number of splits (nodes) and number of leaves of the KD-tree from the statistics printed above, do you think this tree is shallow and wide or narrow and deep, and why?         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    "> 1.           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. If `[100, 1000, 10000, 100000, 1000000]` times as many samples had been used construct and query the KD-tree, what is the expected wall clock tome required for construction and query of the KD-tree? To answer this question you need to compute the compuational complexity for these operations relative to the sames used in the running example. Use the cell below to compute and display a table of the results of your calculation. Your table should have columns showing the multiplier, the expected construction time annd the expected query time.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below \n",
    "run_times = [100, 1000, 10000, 100000, 1000000]\n",
    "n_times = len(run_times)\n",
    "initial_construction_time = 2.4e-3\n",
    "initial_query_time = 2.0e-3\n",
    "n_rows = n_times + 1 \n",
    "\n",
    "time_estimates = pd.DataFrame({'Multiplier':[np.nan]*n_rows, 'Construction Time (sec)':[np.nan]*n_rows, 'Query Time (sec)':[np.nan]*n_rows})\n",
    "time_estimates.iloc[0,:] = [1, initial_construction_time, initial_query_time]\n",
    "\n",
    "for i in range(n_times): \n",
    "    index = i + 1\n",
    "    time_estimates.iloc[index,:] = [run_times[i], run_times[i] * math.log(run_times[i]), math.log(run_times[i])]\n",
    "time_estimates    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. What does the difference in growth of the construction time and query time tell you about the scalability of $n\\ log(n)$ vs. $log(n)$ compuational complexity?  \n",
    "> 3. There is wide range of nearest nearest neighbor distances found in the similarity search. Filtering by a distance threshold has reduced the number of similar candidates. Consider a case where you need to filter 10,000,000 cases down to 100 most similar to present top a user as search results. Given the scalablity of the query do you think this query and filtering can be done in a real-time maner of less than 20 seconds and why?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 2.      \n",
    "> 3.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Sensitive Hashing\n",
    "\n",
    "\n",
    "> **Reference.** This notebook is derived from an example in [Jonathan Kernes' GitHub repository](https://github.com/Jmkernes/Locality-sensitive-hashing-tutorial).\n",
    "\n",
    "> **Note:** A more comprehsnive and more production ready versions of LSH algorithms can be foud in the [DataaSketch](https://ekzhu.com/datasketch/index.html) packge.  \n",
    "\n",
    "In these exercises you will work with mini-hashes and locally-sensitive hashes on a small test dataset. The dataset comprises 4 articles from CNN.com. The 5th item in the dataset is a test document that is a concatenation of sections of 3 of the CNN articles, first, second and fourth. This 5th article is expected to produce artificially high similarity scores. With this small dataset you can easily compare approximations from the hashing methods to the fully Jaccard similarity scores between the text files.  \n",
    "\n",
    "Key steps in this example are: \n",
    "1. Clean and split each text file into a set of K-shingles, using a shingling function.     \n",
    "2. Compute the Jaccard similarity (intersection over union) between all pairs using the full set of shingles, no approximation.   \n",
    "3. Create and apply a MinHashing class which does the following:\n",
    "    - Initialize a dictionary of key-value pairs for the shingles.\n",
    "    - Apply minhashing on a shingle set with \"universal hashing\".\n",
    "    - Create a function to compute a **signature matrix**.\n",
    "4. Evaluate the effectiveness of mini-hashing by computing Jaccard similarity scores of all document pairs.\n",
    "5. Use locally sensitive hashing (LSH) to find **candidate pairs** of high similarity documents. Specifically, use a banded signature matrix to find all pairs whose estimated similarity is above a threshold. A hash table for band and column column ids to enable fast, $O(n)$, comparisons.\n",
    "\n",
    "To begin, execute the code in the cell below to import the required packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Hashing without Locality Sensitive Hashing\n",
    "\n",
    "To begin you will create, execute and evaluate a basic mini-hashing algorithm. This mini-hashing process will then become the basis for the locally sensitive hashing (LSH) algorithm you will work with later in these exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Documents \n",
    "\n",
    "The code in the cell below loads the **corpus** of 5 documents. In text processing a corpus is any collection of related documents. Execute the code to load the document files and display the first 500 characters of each text document.    \n",
    "\n",
    "> **Note:** Depending on your file system, you may need to change the `HOME` and `DIR` variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.getcwd()\n",
    "DIR = 'sampledocs/'\n",
    "TARGET = os.path.join(HOME, DIR)\n",
    "\n",
    "def print_articles(docs, num_characters=500):\n",
    "    for i in range(len(docs)):\n",
    "        print('\\nFirst 500 characters of document ' + str(i) + '\\n')\n",
    "        print(docs[i][:500])    \n",
    "\n",
    "documents = []\n",
    "for article in sorted(os.listdir(TARGET)):\n",
    "#for article in os.listdir(TARGET):\n",
    "    if article == 'stopwords':\n",
    "        continue\n",
    "    path = os.path.join(TARGET, article)\n",
    "    with open(path, 'r') as file:\n",
    "        documents.append(file.read())    \n",
    "       \n",
    "print_articles(documents)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text of these documents appear to have structure common to news articles. The question you will investigate is the similarity of these articles to each other, and particularly the last article, which is comprised of elements of documents, 0, 1 and 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Preparation\n",
    "\n",
    "The first step in determining document similarity is to prepare the document text. Many text preprocessing steps are used in text mining. In this case, we will just do these minimum steps:\n",
    "1. Remove **Stop words** from the text. Stop words are common words very likely to occur multiple times in most any document. Since stop words occur frequently, yet have little or no semantic value, they bias the similarity between documents. Examples of common stop words are 'and' or 'the'. The code in the cell below creates a list of stop words from a file. If a stop word is encountered in the text of one of the documents, the stop word and surrounding white space is replaced by a single white space character, ' '.        \n",
    "2. Remove end of line characters, '\\n', and replace them with white space, ' '.\n",
    "3. Convert all characters to lower case.   \n",
    "\n",
    "Execute the code in the cell below to create and display the list of stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open(os.path.join(TARGET, 'stopwords'), 'r') as file:\n",
    "    for line in file:\n",
    "        stopwords.append(line.strip())\n",
    "        \n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the list of stop words. Notice that they are all common words that generally have low semantic value.   \n",
    "\n",
    "The code in the cell below does the following:    \n",
    "1. Applies the preprocessing steps to each of the documents.    \n",
    "2. Computes and displays some simple summary statistics about the documents.   \n",
    "3. Displays the first 500 characters of the processed documents.    \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    doc = doc.strip().replace('\\n', ' ').lower()\n",
    "    for word in stopwords:\n",
    "        doc = doc.replace(' '+word+' ', ' ')\n",
    "    documents[i] = doc\n",
    "\n",
    "print(f\"Average char-length: \\\n",
    "{np.mean(np.array([len(x) for x in documents]))}\")\n",
    "print(f\"Min char-length: {min(len(x) for x in documents)}\")\n",
    "print(f\"Max char-length: {max(len(x) for x in documents)}\")\n",
    " \n",
    "print('\\n')    \n",
    "print_articles(documents)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several observations one can make about the processed documents:   \n",
    "1. All of the documents are in a similar size range.   \n",
    "2. The preprocessing steps have been successfully carried out with stop words and end of line characters gone and the characters all lower case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shingling Documents\n",
    "\n",
    "Now you are ready to shingle the documents.  \n",
    "\n",
    "> **Exercise 3-2:** You will complete the `getShingles` function below. The function uses a [Python set](https://python-reference.readthedocs.io/en/latest/docs/sets/) to create the shingles for each document. Here, we use character shingles, including white space, number and punctuation. The missing code uses a moving window of length $k$ to find the shingles. Now write code doing the following:    \n",
    "> 1. Loop over the rage of possible overlapping windows of length K.     \n",
    "> 2. In each window add the resulting shingle to the set. You can find a short overview of the Python set add method [here](https://www.w3schools.com/python/python_sets_add.asp).  \n",
    "> 3. Execute your code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create K-shingles by sliding window approach\n",
    "def getShingles(str1, K=5):\n",
    "    d1 = set()\n",
    "    ## Your code goes below\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return d1\n",
    "\n",
    "doc_shingles = [getShingles(s, 5) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the number of resulting shingles is less than the number of windows. Why would this condition arise from using a Python set and what does it mean in terms of occurrence of the shingles in a document from the univerisal set?        \n",
    "> **End of exercise.**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Jaccard similarity      \n",
    "\n",
    "> **Exercise 3-3:** The code in the cell below uses the sets of shingles to compute the Jaccard similarity between the five documents. Two lists are created, one for the pair labels and one for the Jaccard similarity. \n",
    "> You will complete the `jaccardSim` function. The function should return the length of the **intersection** of the data set divided by the length of the **union** of the set. Use the built in Python methods [intersection](https://www.w3schools.com/python/ref_set_intersection.asp) and [union](https://python-reference.readthedocs.io/en/latest/docs/sets/union.html) compute the intersection and the union of the shingle sets.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardSim(d1,d2):\n",
    "    ## Your code goes below\n",
    "    \n",
    "    \n",
    "\n",
    "pair_labels = []\n",
    "pair_sims = []\n",
    "for x1, x2 in itertools.combinations(zip(range(len(doc_shingles)),doc_shingles), 2):\n",
    "    pair_labels.append((x1[0],x2[0]))\n",
    "    pair_sims.append(jaccardSim(x1[1],x2[1]))\n",
    "    \n",
    "print(f\"**True similarity scores**\")\n",
    "print(\"Pair\\tScore\")\n",
    "print(\"-\"*14)\n",
    "for pair, score in zip(pair_labels, pair_sims):\n",
    "    print(f\"{pair}\\t{score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Which 3 of these similarity pairs have the largest Jaccard similarity scores and given the nature of these documents, is this result expected?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a Shingle Dictionary with Universal Set   \n",
    "\n",
    "To prepare for mini-hashing of these documents a shingle dictionary using the universal set of shingles must be created. The code in the cell below does the following:  \n",
    "1. Uses the Python `set.union` method to create a Python with the universal set of shingles.   \n",
    "2. Instantiate a Python dictionary using the universal set as the key values.    \n",
    "3. Print the number of unique shingles found.     \n",
    "Execute this code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the univeral set as the union of all shingles and \n",
    "## then create a dictionary using the universal set as keys. \n",
    "universal_set = set.union(*doc_shingles)\n",
    "shingle_dict = dict(zip(list(universal_set),range(len(universal_set))))\n",
    "print(f\"Number of unique shingles = {len(shingle_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is typical for even small documents, the number of unique shingles is quite large.       \n",
    "\n",
    "Next, execute the code in the cell below to display the first 40 of these shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(shingle_dict.keys())[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is no surprise that each shingle contains 5 characters. Notice that these characters can include white space and punctuation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the HashManager class\n",
    "\n",
    "The MashManager class is used to instantiate and manage the capable of creating a signature matrix. To maintain reasonable performance and minimize memory use on the sparse characteristic matrix, this class **only works with Python sets**, not arrays. Unfortunately, using sets leads to some more difficult to understand code.    \n",
    "\n",
    "Looking at how the provided test cases are executed will help you understand how the methods of this class are used. The methods are:   \n",
    "1. `_initParams`, initializes the signature matrix as random numbers in the range $[0,size\\ of\\ universal\\ set-1]$, using [numpy.random.randint](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html). \n",
    "2. `_permuteRow`, computes a hash of the row indicies used to find permutations of the characteristic matrix.    \n",
    "3. `__call__` does the heavy lifting:  \n",
    "  - Create a new signature matrix if required.  \n",
    "  - Initialize the signature matrix with infinity values, [numpy.inf](https://numpy.org/devdocs/reference/constants.html). \n",
    "  - Loop over all documents and for each shingle in each document updated the signature matrix if the new values of the mini-hash are less that the old values.    \n",
    "\n",
    "Execute the code in the cell below to initialize the HashManager class and run some test cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hash function\n",
    "# define as a callable class, so that we only\n",
    "# intialize random functions once\n",
    "class HashManager():\n",
    "    def __init__(self, shingle_dict):\n",
    "        self.shingle_dict = shingle_dict\n",
    "        self.N = len(shingle_dict)\n",
    "        self.params = None\n",
    "        \n",
    "    def _initParams(self, n_sig):\n",
    "        self.params = np.random.randint(self.N, size=[n_sig,2])\n",
    "    \n",
    "    def _permuteRow(self, row):\n",
    "        return (self.params@np.array([1,row]))%self.N\n",
    "    \n",
    "    def __call__(self, docs, n_sig, init=True):\n",
    "        # Initialize if we change signature matrix length\n",
    "        # or if we request to re-initialize\n",
    "        if self.params is None or len(self.params) != n_sig or init:\n",
    "            self._initParams(n_sig)\n",
    "            \n",
    "        #initialize signature matrix\n",
    "        sig = np.full((n_sig, len(docs)), np.inf)\n",
    "        \n",
    "        # each doc in docs is assumed to be an iterable object\n",
    "        for j, doc in enumerate(docs):\n",
    "            for shingle in doc:\n",
    "                orig_row = shingle_dict[shingle]\n",
    "                curr_col = self._permuteRow(orig_row)\n",
    "                sig[:,j] = np.minimum(sig[:,j],curr_col)\n",
    "        return sig.astype(int)\n",
    "    \n",
    "# run some tests:\n",
    "try:\n",
    "    print(\"Initialization test: \", end=\"\")\n",
    "    hm = HashManager(shingle_dict)\n",
    "    print(\"passed\")\n",
    "\n",
    "    print(\"Set parameters to right size: \", end=\"\")\n",
    "    hm._initParams(n_sig=4)\n",
    "    assert(hm.params.shape == (4,2))\n",
    "    print(\"passed\")\n",
    "\n",
    "    print(\"Permuting a row integer returns array: \", end=\"\")\n",
    "    curr_col = hm._permuteRow(3)\n",
    "    assert(curr_col.shape == (4,))\n",
    "    print(\"passed\")\n",
    "\n",
    "    print(\"Compute minhashed signature matrix: \", end=\"\")\n",
    "    hm(doc_shingles, 4)\n",
    "    print(\"passed\")\n",
    "except Exception as e:\n",
    "    print(\"failure\")\n",
    "    print(e.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `HashManager` class defined, execute the code in the cell below to create an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4856)\n",
    "hm = HashManager(shingle_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Mini-Hashing Performance\n",
    "\n",
    "Now you will compute and compare similarity scores between the simple mini-hash approximation and the full Jaccard similarity using the shingled documents. \n",
    "\n",
    "The following functions are provided:   \n",
    "1. `trueSimScores` function computes the Jaccard similarities of each possible combination of the shingled documents.   \n",
    "2. `sigSimScores` function computes the Jarracd similarities between each possible combination of mini-hashes of a document.  \n",
    "3. `candidatePairs` function finds combinations of mini-hashes that have a similarity greater than the threshold.  \n",
    "4. `printScoreComparison` function prints the comparison between the mini-hash similarity and the full Jaccard similarity. Similarities computed by Jaccard similarity on the full set of shingles are assumed to be the ground truth.  \n",
    "5. `accMatrix`function prints some summary statistics comparing the similarity methods.    \n",
    "\n",
    "Execute this code and examine the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trueSimScores(doc_shingles):\n",
    "    pair_labels = []\n",
    "    pair_sims = []\n",
    "    idxs = range(len(doc_shingles))\n",
    "    for x1, x2 in itertools.combinations(zip(idxs,doc_shingles), 2):\n",
    "        pair_labels.append((x1[0], x2[0]))\n",
    "        pair_sims.append(jaccardSim(x1[1], x2[1]))\n",
    "    return dict(zip(pair_labels, pair_sims))\n",
    "    \n",
    "def sigSimScores(sig_mat):\n",
    "    cols = sig_mat.T\n",
    "    idxs = range(sig_mat.shape[1])\n",
    "    \n",
    "    pair_labels = []\n",
    "    pair_sims = []\n",
    "    for (i,col1), (j,col2) in itertools.combinations(zip(idxs, cols),2):\n",
    "        pair_labels.append((i,j))\n",
    "        pair_sims.append(np.mean(col1==col2))  \n",
    "    return dict(zip(pair_labels, pair_sims))\n",
    "\n",
    "def printScoreComparison(true_dict, approx_dict):\n",
    "    print(f\"Comparison of similarity scores between mini-hash approximation and full Jaccard similarity\")\n",
    "    print(\"Pair\\t\\tApprox\\t\\tTrue\\t\\t%Error\")\n",
    "    errors =[]\n",
    "    for pair, true_value in true_dict.items():\n",
    "        approx_value = approx_dict[pair]\n",
    "        err = abs(true_value-approx_value)/true_value\n",
    "        errors.append(err)\n",
    "        print(f\"{pair}\\t\\t{approx_value:.3f}\\t\\t{true_value:.3f}\\t\\t{100*err:.2f}\")\n",
    "    print(f\"Variance = {np.var(errors)}\")    \n",
    "\n",
    "def candidatePairs(score_dict, threshold):\n",
    "    return set(pair for pair, scr in score_dict.items() if scr>=threshold)\n",
    "\n",
    "def accMatrix(true_dict, approx_dict, threshold):\n",
    "    true_pairs = candidatePairs(true_dict, threshold)\n",
    "    approx_pairs = candidatePairs(approx_dict, threshold)\n",
    "   \n",
    "    false_negatives = len(true_pairs.difference(approx_pairs))\n",
    "    false_positives = len(approx_pairs.difference(true_pairs))\n",
    "    \n",
    "    print('\\n')\n",
    "    print(f\"False negatives: {false_negatives}\")\n",
    "    print(f\"Potential false positives: {false_positives}\")\n",
    "\n",
    "np.random.seed(9844)\n",
    "sig_mat = hm(doc_shingles, 10)\n",
    "true_score_dict = trueSimScores(doc_shingles)\n",
    "approx_score_dict = sigSimScores(sig_mat)\n",
    "printScoreComparison(true_score_dict, approx_score_dict)\n",
    "\n",
    "threshold = 0.25\n",
    "print('\\n\\nComparison of similarity measure methods at threshold = ' + str(threshold))\n",
    "print(\"Pairs by full Jaccard similarity:\",candidatePairs(true_score_dict, threshold))\n",
    "print(\"Candidate pairs from mini-hashing:\",candidatePairs(approx_score_dict, threshold))\n",
    "accMatrix(true_score_dict, approx_score_dict, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3-4** Provide short answers to the following questions:    \n",
    "> 1. Tables of performance statistics are printed above. The first table compares the pair-wise similarity measures. The second table compares the similar pairs found by both methods. What do these summaries tell you about the variance of the mini-hash approximation and why is this result expected.\n",
    "> 2. Based on the results displayued, explain the false negatives and false positives counts given the pairs found by the full Jaccard similarity and mini-hashing.      \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.     \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing\n",
    "\n",
    "### Banding Signature Matrix\n",
    "\n",
    "You have just seen that the basic mini-hashing algorithm has high variance. Algorithms which band the signature matrix can significantly reduce this variance, by averaging out independent errors. Mini-hash estimates of similarity are computed for each of $b$ bands of the signature matrix. If the signature matrix has $n$ rows, we divide it into $b$ bands each of width $r$, such that $n = b*r$.\n",
    "\n",
    "Let $p$ be the true similarity score (match percent) between a pair. The probability of matching every hash in a band is:\n",
    "\n",
    "$$\\text{prob. all hashes match } = p^r$$\n",
    "\n",
    "The probability of one of $r$ hashes mismatch is:  \n",
    "\n",
    "$$\\text{prob. one hash mismatch } = 1 - p^r$$\n",
    "\n",
    "Now, the probability that at least 1 of the 𝑟 mini-hashes disagree in the 𝑏 bands:\n",
    "\n",
    "$$\\text{prob. no bands match } = (1-p^r)^b$$\n",
    "\n",
    "Therefore, the probability that the $b$ bands do match, is:\n",
    "\n",
    "$$P(\\geq 1\\text{ match}) = 1-(1-p^r)^b$$\n",
    "\n",
    "For a decision threshold of $1/2$, the decision criteria used is that two variables are a candidate pair if $p \\geq 1/2$. For a fixed value of $n$, the number of mini-hashes, one can tune the mini-hash by choosing $r$ and $b$ to approximately a step function around the true value of $p$. The goal is to find all true candidate pairs while minimizing false positive detections. This requirement dictates that some analysis is required to optimize for specific problems.  \n",
    "\n",
    "The code in the cell below computes and displays the decision curves for a fixed $n$ for various values of ${r,b}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 200\n",
    "ops = [(2,100),(4,50),(10,20),(20,10),(50,4),(100,2)]\n",
    "yval = lambda p,r,b: 1-(1-p**r)**b\n",
    "pts = np.linspace(0,1,100)\n",
    "for op in ops:\n",
    "    plt.plot(pts, yval(pts,op[0],op[1]), label=op)\n",
    "plt.hlines(0.5,0,1, linestyles ='dashed', colors ='k')\n",
    "plt.vlines(0.5,0,1, linestyles ='-.', colors ='k')\n",
    "plt.legend()\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(\"legend: (r,b). p_true=0.5 (vertical line)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3-5:** Examine the curves in the plot and provide short . \n",
    "> 1. How does the position of the curve change with values of r and b for a fixed n and how does this change the optimal decision threshold?\n",
    "> 2. How will the rate of true positives and false positives change with r and b? Hint, note how the steepness of the curve changes with r and b.    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.    \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal parameters\n",
    "\n",
    "Consider the variables for the decision problem of determining which pairs of variables are similar:  \n",
    "- $p$, the **crossover point** or **decision point**, values above p are considered similar.   \n",
    "- $b$, the number of bands of the signature matrix.  \n",
    "- $r$, the number of mini-hashes in each band.  \n",
    "- $n$, the number of samples, with $n=br$.\n",
    "\n",
    "For a fixed $n, p$ it is possible to solve for the optimal values r,b. Conversely of a fixed $n,r,b$ we can determine the crossover point $p$ for the decision, $p=f(b,r)$. Let's start with $p=1/2$.\n",
    "$$1/2 = 1-(1-p^r)^b$$\n",
    "$$ 1-p^r = 2^{-1/b}$$\n",
    "$$p = (1-2^{-1/b})^{1/r} = (1-e^{-(1/b)\\ln2})^{1/r} \\approx (1/b)^{1/r}*\\text{const}$$\n",
    "\n",
    "If we fix $r$ and $p$, we can find the required number of bands to be about\n",
    "$b \\approx 1/p^r$\n",
    "\n",
    "You have already scene how changing $r$ and $b$ for a fixed $n$ shifts the decision curve. The code in the cell below plots the decision function curve for several optimal values of $r,b$ with increasing $n$. The values of $r$ are considered optimal since the crossing point, $p=0.5, for each combination. Execute the code and examine the results.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100) \n",
    "for b,r,col in [(5,3,'r'),(40,6,'b'),(150,8,'g'),(700,10,'b')]:\n",
    "    y = 1-(1-x**r)**b\n",
    "    plt.plot(x,y,c=col,label=f\"b={b},r={r}, n={b*r}\")\n",
    "plt.hlines(0.5,0,1, linestyles ='dashed', colors ='k')\n",
    "plt.vlines(0.5,0,1, linestyles ='-.', colors ='k')\n",
    "plt.legend()\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3-6:** Keeping in mind that a steeper curve is a better decision function, for optimal $r,b$, what do these curves tell you about the value of increasing $n$.    \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH with Banded Mini-Hash \n",
    "\n",
    "Let's try a simple example of banded mini-hashing. Specifically, the OR-AND method is applied to a banded signature matrix.       \n",
    "The code in the cell below implements the algorithm is a straight-forward way. While not optimal in terms of computational complexity, the code is intended to make the algorithm clear. The functions provided are:    \n",
    "1. `bandedCandidatePair`, is a function which first computes a logical array of matches between two columns, then divides that array into $b$ bands and determines if any of these bands are all true values, or all matches. The signature matrix is divided into bands using [numpy.array_split](https://numpy.org/doc/stable/reference/generated/numpy.array_split.html). \n",
    "2. `bandedCandidatePairs`, calls the bandedCandidatePair function for each combination of columns, and if that function returns a logical true (a match of at least one band) the keys for the pair are added to the pairs list.\n",
    "\n",
    "Execute the code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandedCandidatePair(col1, col2, b, r):\n",
    "    \"\"\"Returns a boolean if the two columns are a candidate pair\n",
    "    inputs must obey n=len(col1)=len(col2)=b*r\"\"\"\n",
    "    n = len(col1)\n",
    "    assert(n==b*r)\n",
    "    assert(n==len(col2))\n",
    "    truth_array = (col1==col2)\n",
    "    return any(all(band) for band in np.array_split(truth_array,b))\n",
    "\n",
    "def bandedCandidatePairs(sig_mat, b, r):\n",
    "    d = sig_mat.shape[1]\n",
    "    idxs = range(d)\n",
    "    cols = [sig_mat[:,i] for i in range(d)]\n",
    "    pairs = set()\n",
    "    for (i,col1), (j,col2) in itertools.combinations(zip(idxs,cols),2):\n",
    "        if bandedCandidatePair(col1,col2,b,r):\n",
    "            pairs.add((i,j))\n",
    "    return pairs\n",
    "\n",
    "# set p = 0.3 arbitrarily\n",
    "p = 0.25\n",
    "n = 120\n",
    "b = 20\n",
    "r = 6\n",
    "\n",
    "# see how many candidate pairs we got right!\n",
    "np.random.seed(57988)\n",
    "sig_mat = hm(doc_shingles, n)\n",
    "true_score_dict = trueSimScores(doc_shingles)\n",
    "\n",
    "print(\"\\nPairs found by Jaccard similarity of all shingles:\",candidatePairs(true_score_dict, p))\n",
    "print(\"Pairs found with LSH:\",bandedCandidatePairs(sig_mat, b, r))\n",
    "print(\"Simple MinHash pairs:\",candidatePairs(approx_score_dict, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-7:** Provide short answers to the following questions:    \n",
    "> 1. Examine the code in the `return` statement of the `bandedCandidatePair` function. Explain why this code implements the OR-AND construction algorithm.  \n",
    "> 2. Compare the candidate pairs found by the three methods. Given the composition of document 4 which errors are made in terms of false positive and false negative by each of the methods, and what does this result tell you about their respective accuracies? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.     \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient LSH algorithm  \n",
    "\n",
    "The banded OR-AND mini-hashing has produced better results than simple mini-hashing. However, the algorithm used required matching $\\frac{n(n-1)}{2}$ or $\\mathcal{O}(n^2)$ matching operations. Clever use of a hash table allows us to create an equivalent algorithm with  $\\mathcal{O}(n)$ time complexity.   \n",
    "\n",
    "The code in the cell below uses a hash table constructed with a [collections.defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) to match candidate pairs. Execute the code and examine the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastCandidatePairs(sig_mat, b, r):\n",
    "    n, d = sig_mat.shape\n",
    "    assert(n==b*r)\n",
    "    hashbuckets = collections.defaultdict(set)\n",
    "    bands = np.array_split(sig_mat, b, axis=0)\n",
    "    for i,band in enumerate(bands):\n",
    "        for j in range(d):\n",
    "            # The last value must be made a string, to prevent accidental\n",
    "            # key collisions of r+1 integers when we really only want\n",
    "            # keys of r integers plus a band index\n",
    "            band_id = tuple(list(band[:,j])+[str(i)])\n",
    "            hashbuckets[band_id].add(j)\n",
    "    candidate_pairs = set()\n",
    "    for bucket in hashbuckets.values():\n",
    "        if len(bucket) > 1:\n",
    "            for pair in itertools.combinations(bucket, 2):\n",
    "                candidate_pairs.add(pair)\n",
    "    return candidate_pairs\n",
    "\n",
    "\n",
    "#p = 0.30\n",
    "p = 0.25\n",
    "n = 120\n",
    "b = 20\n",
    "r = 6\n",
    "\n",
    "# see how many candidate pairs we got right!\n",
    "np.random.seed(1267)\n",
    "sig_mat = hm(doc_shingles, n)\n",
    "true_score_dict = trueSimScores(doc_shingles)\n",
    "approx_score_dict = sigSimScores(sig_mat)\n",
    "print('True pairs:\\t', candidatePairs(true_score_dict, p))\n",
    "print(\"LSH pairs:\\t\", bandedCandidatePairs(sig_mat, b, r))\n",
    "print(\"Fast LSH pairs:\\t\", fastCandidatePairs(sig_mat, b, r))\n",
    "print(\"MinHash pairs:\\t\", candidatePairs(approx_score_dict, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-8:** Examine the `fastCandidatePairs` function. Explain why this algorithm is equivalent to the OR-AND construction examined for Exercise 03-6, but yet can run in $O(n)$ time.    \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-9:** The LSH and fast LSH algorithms both make the same error in this case. Is there an efficient way to improve on this result? Yes. We can take advantage of the fact that most similar pairs hash to the same bucket, we can do the following:    \n",
    "> 1. Find the full Jacard similarities of only the pairs in the bucket. The limited number of pairs candidates makes this calculation efficient.    \n",
    "> 2. Filter the full Jarard similarities by a threshold.       \n",
    "> Examine and execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_labels = []\n",
    "pair_sims = []\n",
    "for x1, x2 in bandedCandidatePairs(sig_mat, b, r):\n",
    "    pair_labels.append((x1,x2))\n",
    "    pair_sims.append(jaccardSim(doc_shingles[x1],doc_shingles[x2]))\n",
    "    \n",
    "print('The similaritiy of the pairs in the bucket = ' + str(np.round(pair_sims, 3)))\n",
    "threshold = 0.25\n",
    "print(f\"**True similarity scores**\")\n",
    "print(\"Pair\\tScore\")\n",
    "print(\"-\"*14)\n",
    "for pair, score in zip(pair_labels, pair_sims):\n",
    "    if score > threshold: print(f\"{pair}\\t{score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This simple algorithm has filtered out false positive errors. Explain why this approach is compuationally efficeint compare to testing pairwise similarities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copywrite 2022, 2023, 2024 Stephen F Elston. All rights reserved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
