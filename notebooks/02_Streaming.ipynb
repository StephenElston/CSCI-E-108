{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cb284d",
   "metadata": {},
   "source": [
    "# Assignment 02     \n",
    "## CSCI S-96    \n",
    "### Steve Elston\n",
    "\n",
    "> **Instructions:** For this assignment you will complete the exercises shown. All exercises involve creating and executing some Python code. Additionally, most exercises have questions for you to answer. You can answer questions by creating a Markdown cell and writing your answer. If you are not familiar with Markdown, you can find a brief tutorial [here](https://www.markdownguide.org/cheat-sheet/).   \n",
    "\n",
    "In this assignment you will work with some basic streaming analytic algorithms. To avoid the complexities of installing and setting up a real streaming analytics platform,you will work with stream flow data loaded from local files. Specifically in this assignment you will:    \n",
    "1. Create and apply code to perform basic stream queries.    \n",
    "2. Using stream queries and plots, explore the stream data.    \n",
    "3. Use moving windows to compute moving averages and sub-sample a stream.    \n",
    "4. Use exponential decay filters to compute moving averages and sub-sample a stream.    \n",
    "5. Work with a quotient filter type filter to filter for customer identifiers on a list.   \n",
    "\n",
    "## Overview \n",
    "\n",
    "The United States Geological Survey (USGS) maintains over 13,500 stream flow gages in the United States. Measurements from most of these gages are recoded every 15 min and uploaded every 4 hours are [available for download](https://waterdata.usgs.gov/nwis/rt). Stream flow data are used as inputs for complex water management tasks for purposes such as agriculture, residential use and conservation. For this assignment you will work with the time series of measurements for two stream flow gages on tributaries of the the Columbia River in the US State of Washington.     \n",
    "\n",
    "To get started, execute the code in the cell below to import the packages you will need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ad9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import mmh3\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff02ac",
   "metadata": {},
   "source": [
    "## Loading the Dataset  \n",
    "\n",
    "The next step is to load the stream gage data. The code in the cell below loads the time series data for the first gage. This gage is sited on the Okanogan river.  \n",
    "\n",
    "The code in the cell below does the following:  \n",
    "1. Loads the data from a .csv file. \n",
    "2. Converts the time stamp column to an index of the Pandas data frame. \n",
    "3. Assigns human-understandable names to the columns.  \n",
    "4. Returns just the first 4 columns of the data frame. \n",
    "\n",
    "Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afbed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_index_series(file_name):  \n",
    "    '''Function to read time series data from a file.\n",
    "    Argument is the path and filename.'''\n",
    "    df = pd.read_csv(file_name, sep='\\t')\n",
    "    df.index = df.datetime\n",
    "    df.drop('datetime', axis=1, inplace=True)\n",
    "    df = df.iloc[:,:4]\n",
    "    df.columns = ['Agency', 'Site_number', 'Time_zone', 'Stream_flow']\n",
    "    return df.iloc[:,:4]\n",
    "\n",
    "Malott = read_index_series('../data/12447200_Okanogan_at_Malott.txt')\n",
    "Malott"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12aa937",
   "metadata": {},
   "source": [
    "The other time series is for a gage on the Yakima River. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleElm = read_index_series('../data/12479500_Yakima_At_CleElm.txt')\n",
    "CleElm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7b2f4",
   "metadata": {},
   "source": [
    "Since we really only want to work with one data frame. The code in the cell below merges the two time series and sorts them into time index order. Execute this code and examine the result, paying attention to the site number and the datetime index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ccdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_flow = Malott.append(CleElm).sort_index()\n",
    "stream_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb8b33",
   "metadata": {},
   "source": [
    "## Querying Stream Data\n",
    "\n",
    "Common stream data operations are often formulated as queries on the stream data. Many streaming data platforms use extensions of SQL for these queries.   \n",
    "\n",
    "To keep things simple in this assignment we will just use a simple query function. The function shown in the cell below supports simple queries on the    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66133e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def query_stream(df, Columns=None, site_numbers=None, start_time=None, end_time=None):    \n",
    "    '''\n",
    "    Function to query the stream gage time series data. The arguments are:    \n",
    "    df = the data frame containing the data.  \n",
    "    Columns = a list of columns to return.   \n",
    "    site_numbers = a list of gage site numbers to query data. \n",
    "    start_time = the start time of the returned data as datatime string or integer index.   \n",
    "    end_time = the end time of the returned data as datatime string or integer index.\n",
    "    '''\n",
    "    ## First set values for arguments set to Null  \n",
    "    if Columns==None: Columns = df.columns\n",
    "    if start_time==None: start_time = df.index[0]\n",
    "    if end_time==None: end_time = df.index[df.shape[0]-1]\n",
    "    if site_numbers==None: site_numbers = df.Site_number.unique()\n",
    "    ## Test if index is a string datetime or an integer\n",
    "    ## use iloc method if an integer.\n",
    "    ## A slice over the time range is created based on the index type. \n",
    "    if isinstance(start_time, str):\n",
    "        df = df.loc[start_time:end_time,:]\n",
    "    else:     \n",
    "        df = df.iloc[start_time:end_time,:]\n",
    "    df = df.loc[df.Site_number.isin(site_numbers), Columns]\n",
    "    ## Return the results of the query\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31bf56",
   "metadata": {},
   "source": [
    "You can see the options to run `query_stream` function by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_stream.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daad7f",
   "metadata": {},
   "source": [
    "An example of using the query function is shown in the cell below. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff424adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c9f9",
   "metadata": {},
   "source": [
    "> **Exercise 02-01:** Using the `query_stream` function, write and execute the code in the cell below to compute and display the mean `Stream_flow`for the month of April of 2020 of site 1248500. Use the [Pandas.DataFrame.mean](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html) method to compute the mean. Notice that using this approach we can compute most any statistic of interest on the query result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc98d30",
   "metadata": {},
   "source": [
    "## Plotting Streaming Data\n",
    "\n",
    "Visualization is important tool in data exploration and discovery. Numerical stream data is ideal for visual exploration if it can be subsampled to manageable size.  \n",
    "\n",
    "The function in the cell below creates a time series plot. The time index of the Pandas data frame is used to generate the x-axis values. Execute the code in this cell to load this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768467d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df, ax=None, ylabel='Stream flow', title=''): \n",
    "    if ax==None: fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    df.plot(ax=ax);\n",
    "    ax.set_xlabel('Date');\n",
    "    ax.set_ylabel(ylabel);\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea571c",
   "metadata": {},
   "source": [
    "The code in the cell below creates time series plots of the stream flow data. The flow time series for two stream gages queried as arguments to the plot function. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e82d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12447200]), title='Flow on Okanogan a tMalott') \n",
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]), title='Flow on Yakima at Cle Elm')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c53ca",
   "metadata": {},
   "source": [
    "The time series of stream flow at both of these gages is rather complex. Both rivers have several dams used to control the flow. The flow is optimized to conserve fisheries and to supply agriculture in the Columbia River Basin. Water in reservoirs accumulates in the spring as mountain snow melts. The water is then released throughout the spring and summer. \n",
    "\n",
    "But, what can we make of the noticeable spikes in flow, particularly for gage $12484500$ on the Yakima River. Even with the control provided by dams and reservoirs spring and early summer storm events can cause temporary increases in water flow. These storms bring heavy, and often warm, rain. Flow in the rivers increases not only because of the rainfall, but also since warm rain accelerates snow melt in the higher elevations.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e06c6f",
   "metadata": {},
   "source": [
    "> **Exercise 02-02:** The transitory flow events on the Yakima River warrant some further investigation. You now have the tools to query and plot the stream flow time series. Your goal is to determine if there are common properties (e.g. duration or amplitude) of these events. Plot the results of a query for stream flows on gage $12484500$, Yakima River, from the 6th day of April to the 20th day of June, 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4447407",
   "metadata": {},
   "source": [
    "> Discuss any common pattern in terms of approximately common amplitudes or durations of these events you can see.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ccc59",
   "metadata": {},
   "source": [
    "> **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8589b",
   "metadata": {},
   "source": [
    "## Applying Moving Window Filters\n",
    "\n",
    "Moving window filters are a commonly used method to compute statistical samples from streaming data. \n",
    "\n",
    "Apply a moving window filter. \n",
    "\n",
    "> **Exercise 02-03:** You will complete and test the function in the cell below. The function queries a time series to create overlapping windows of a specified length and stride. For each window the mean of the stream flow is computed. The function returns a [Pandas Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object. The time index of the Series object is the mid-point index of the window used to compute the statistic. \n",
    "> 1. Use a `for` loop to create the overlapping moving window samples of the input. The window will use the `length` and `stride` arguments to the function. At each iteration, the window will advance by `stride` time steps. See the [tutorial on the Python range function](https://www.w3schools.com/python/ref_func_range.asp) for help.    \n",
    ">   - Query the input stream data for the stream flow values in the window. The `query_stream` function will accept integer indices for the `start_time` and `end_time` arguments. Make sure the these indicies are within the range of the original time series.  \n",
    ">   - Append the mean of the stream flow values in the window to the `out` list.\n",
    ">   - Append the time index of the midpoint of the window to the `idx` list.  \n",
    "> 2. Once the loop has terminated used the [Pandas.Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) constructor to instantiate the return series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_average(ts, length=16, stride=8, Columns='Stream_flow', site_numbers=[12484500]):\n",
    "    half_length = int(length/2)-1\n",
    "    idx = []\n",
    "    out = []\n",
    "    ## Put your code below\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    out = pd.Series(out, index=idx)    \n",
    "    return out     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be829a8",
   "metadata": {},
   "source": [
    "> 3. Next you will test your function by completing and executing the code in the cell below. Use your `window_average` function to create a Pandas Series with 4-hour stream flow averages (length of 16 time steps), taken every 2 hours (stride of 8 time steps). Name your Series filtered_12 and compute and print the length. The code provided queries the data so that you are working with only values from the Yakima River gage. Using flow rate values from only one gage simplifies the bookkeeping for window sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query to create a series with only the Yakima River stream gage data. \n",
    "\n",
    "filtered_12 = window_average(Yakima)\n",
    "len(filtered_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca04cf",
   "metadata": {},
   "source": [
    "> Notice how the length of the time series has been significantly reduced. Is the reduction in length of the time series consistent with a stride of 8 time steps?   \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3992f0",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945f177",
   "metadata": {},
   "source": [
    "To examine the 4-hour moving average time series you have computed, execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "_=plot_time_series(filtered_12, title='Flow on Yakima at Cle Elm. 4-hour average')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0b31d",
   "metadata": {},
   "source": [
    "> **Exercise 02-4:** You will compute and display a time series using a longer, 1-day (96 time steps) moving window with a stride of 1/2 of a day (48 time steps). For this exercise, do the following:   \n",
    "> 1. Query the Yakima River stream gage data with the longer time window and stride. \n",
    "> 2. Print the length of the resulting Pandas Series.  \n",
    "> 3. Plot the moving average series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ebcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "print(len(filtered_96))\n",
    "_=plot_time_series(filtered_96, title='Flow on Yakima at Cle Elm, 1-day average')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d9f87",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> A. What is the data compression ratio of the longer time window? Is this consistent with the stride of the window?   \n",
    "> B. Compare the plots of the results of the two moving window summaries. What are the obvious differences? \n",
    "> 3. If the goal is to measure total volume of water passing the gage on a daily and weekly basis, does the series from the longer filter contain sufficient detail? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f22026",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.          \n",
    "> 2.     \n",
    "> 3.                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438bb96b",
   "metadata": {},
   "source": [
    "## Exponential Decay Filters\n",
    "\n",
    "The idea of using exponential smooth for time series analysis is an old one, dating at least to use by Weiner in the 1920s. The related idea of moving average filters was developed by Kolmogorov and Zurbenko in the 1940s. Exponential smoothers were used extensively in signal process in the 1940s. The general idea was expanded by Robert Goodell Brown (1956) and C.E. Holt (1957) and his student P.R. Winters (1960). The higher-order Holt-Winters model accounts for trend and seasonality of time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c336514",
   "metadata": {},
   "source": [
    "### Basic exponential Smoothing\n",
    "\n",
    "Exponential smoothing uses a weighted sum of the current observation and the past smoothed value to compute a new smoothed value. This basic exponential smoothing relationship is shown below.  \n",
    "\n",
    "$$\n",
    "s_0 = x_0 \\\\\n",
    "s_t = \\alpha x_t + (1-\\alpha) s_{t-1} = s_{t-1} + \\alpha(x_t - s_{t-1}),\\ t \\gt 0\n",
    "$$\n",
    "\n",
    "The smoothing hyperparameter, $\\alpha$, controls the trade-off between the last observation and the previous smoothed values. The possible values are in the range, $0 \\le \\alpha \\le 1$. A large value of $\\alpha$ puts more weight on the current observation. Whereas, a small value of $\\alpha$ puts more weight on the smoothed history.      \n",
    "\n",
    "How can we understand the exponential decay of the smoothed history of a time series? The smoothing hyperparameter, $\\alpha$, an be expressed in terms of the decay constant, $\\tau$ and time interval $\\Delta T$ as shown below.  \n",
    "\n",
    "$$\n",
    "\\alpha = 1 - e^{\\big( \\frac{- \\Delta T}{\\tau} \\big)}\n",
    "$$\n",
    "\n",
    "From this relationship you can see that the influence of the smoothed history decays exponentially as $\\delta T$ increases. The decay time increases as $\\tau$ decreases.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e56565",
   "metadata": {},
   "source": [
    "### Smoothing with higher-order terms   \n",
    "\n",
    "The basic exponential smoothing algorithm is effective in many cases. However, the simple first order exponential smoothing method cannot accommodate time series with trend or seasonality. Higher order smoothing models are required.   \n",
    "\n",
    "The **double exponential smoothing** or **Holt-Winters double exponential smoothing** algorithm is a second order smoothing method. Using two coupled difference equations a trend and non-seasonal component of the time series can be modeled. The model updates a smoothed measure of the non-seasonal component and the trend.   \n",
    "\n",
    "The model is initialized with the values:   \n",
    "$$\n",
    "s_1 = x_1 \\\\\n",
    "b_1 = x_2 - x_1\n",
    "$$\n",
    "\n",
    "At each time step the a pair of time difference equations are updated. The following relationships update the smoothed non-seasonal component, $s_t$, and the slope, $b_t$:   \n",
    "\n",
    "$$\n",
    "s_t = \\alpha x_t + (1-\\alpha) (s_{t-1} + b_{t-1}) \\\\\n",
    "b_t = \\beta(s_t - s_{t-1}) + (1 - \\beta)b_{t-1}\n",
    "$$\n",
    "\n",
    "The smoothed non-seasonal component and smoothed slope can be used to compute a forecast. The relationship below computes the forecast $m$ time steps ahead.      \n",
    "\n",
    "$$ F_{t+m} = s_t + m b_t $$   \n",
    "\n",
    "What about seasonal components? A third-order difference relationship can updated a smoothed seasonal component, along with the smoothed non-seasonal and slope components. The details of this process are not discussed further here. The details are available elsewhere, including the [exponential smoothing Wikipedia page](https://en.wikipedia.org/wiki/Exponential_smoothing#:~:text=Exponential%20smoothing%20is%20a%20rule,exponentially%20decreasing%20weights%20over%20time.).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4642b7",
   "metadata": {},
   "source": [
    "### Example of Exponential Decay Filtering     \n",
    "\n",
    "> **Exercise 02-5:** You will now create and test a single or simple exponentially weighted decay filter. This function will have a stride argument just as the window filter function. Your function, `exponential_smooth`, will have arguments of the time series, the exponential smoothing parameter and a stride. Your function will do the following:    \n",
    "> 1. Save the index of the incoming time series to a variable.   \n",
    "> 2. Initialize the an empty index list for the output series.  \n",
    "3. Initialize the value list for the samples. The samples list will contain the exponentially weighted smoothed samples. Make sure you save the first value in the list.   \n",
    "> 4. Initialize an empty output value list.   \n",
    "> 5. A for loop iterates over all the values of the time series starting with the second one. In this case a query is not used since for a live stream the exponential decay filter is updated each time a value arrives.    \n",
    ">   - Append the computed exponentially weighted values smoothed values to the samples list. \n",
    ">   - If the loop index modulo the stride is 0 then, append the sample value to the output list and the index of that sample to the index list.  \n",
    "> 8. Create an output Pandas Series from the output list and the output index list. \n",
    "> 9. Return the Pandas Series.  \n",
    "> 10. Execute your function with for site number $12484500$ and default arguments of `alpha=0.01` and `stride=8`, 2 hours. \n",
    "> 11. Print the length the resulting series. \n",
    "> 12. Plot the series with the `plot_time_series` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68414cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smooth(ts, alpha=0.01, stride=8):\n",
    "    in_index = ts.index\n",
    "    idx = []\n",
    "    samples= [ts[0]]\n",
    "    out = []\n",
    "    ## Put your code below\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return out       \n",
    "\n",
    "\n",
    "smoothed_01 = exponential_smooth(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]))\n",
    "\n",
    "print(len(smoothed_01))\n",
    "_=plot_time_series(smoothed_01, title='Flow on Yakima at Cle Elm, EWMA filter, alpha=0.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f002f",
   "metadata": {},
   "source": [
    "> Provide short answers to the following questions:   \n",
    "> 1. Are the number of smoothed samples correct for the stride of the exponential decay filter selected?    \n",
    "> 2. Are the details of the filtered time series substantially the same as the original series?  \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bacf3",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.       \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1a1de",
   "metadata": {},
   "source": [
    "> **Exercise 02-6:** A question we should ask is what happens if you increase the smoothing constant of the exponential decay filter? In other words, what is the effect of giving greater weight to past values? To find out do the following:  \n",
    "> 1. Execute the `exponential_smooth` function with arguments `alpha=0.99` and `stride=8`.\n",
    "> 2. Print the length of the resulting Pandas Series. \n",
    "> 3. Plot the smoothed time series using the `plot_time_series` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "print(len(smoothed_98))\n",
    "_=plot_time_series(smoothed_98, title='Flow on Yakima at Cle Elm, EWMA filter, alpha=0.99')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212c4ba",
   "metadata": {},
   "source": [
    "> Compare the plot of this time series to the previous series less smoothing and provide short answers to the following questions.   \n",
    "> 1. What is the main difference you can see between these series?    \n",
    "> 2. WHy is this result expected from the theory?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c493d",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.       \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1cda9",
   "metadata": {},
   "source": [
    "> **Exercise 02-7:** The next question to ask is what is the effect of changing the stride? A longer stride reduces the number of smoothed samples used for further processing. To find out do the following:  \n",
    "> 1. Execute the `exponential_smooth` function with arguments `alpha=0.99` and `stride=96`, one day or 24 hours.\n",
    "> 2. Print the length of the resulting Pandas Series. \n",
    "> 3. Plot the smoothed time series using the `plot_time_series` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "print(len(smoothed_99))\n",
    "_=plot_time_series(smoothed_99, title='Flow on Yakima at Cle Elm, EWMA filter, alpha=0.01, stride=96 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7ad21",
   "metadata": {},
   "source": [
    "> Provide short answers to the following questions. \n",
    "> 1. Compare the plot of this time series to the previous series with a shorter stride. What is the main difference you can see? \n",
    "> 2. Do you think any difference is significant in terms of managing water flow on a daily or weekly basis? \n",
    "> 3. Is the number of samples consistent with the stride defined?       \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ecdb04",
   "metadata": {},
   "source": [
    "> **Answers:**           \n",
    "> 1.       \n",
    "> 2.        \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6046f",
   "metadata": {},
   "source": [
    "## Filtering Events  \n",
    "\n",
    "In many cases our goal is to filter events based on some type of identifier. The identifier is a hash of nearly any hashable data type. Many examples of these data types include, string customers identifiers, numeric event identifiers, IP addresses, email addresses.  \n",
    "\n",
    "One example of such a method is the [**Bloom filter**](https://en.wikipedia.org/wiki/Bloom_filter). A Bloom filter is extremely  However, the Bloom filter does not allow for deleting matches once they are added to the hash table. An alternative is a **[quotient filter](https://en.wikipedia.org/wiki/Quotient_filter)**. The quotient filter keeps a count of events for each hash. As a result, an event identifier can be removed from the table by decrementing the counts for the hashes. To perform this extra operation the quotient filter uses more memory and is a bit less computationally efficient.   \n",
    "\n",
    "Both Bloom filters and quotient filters operate by the same principle. A hash table of key-values pairs is created. The keys are the hashes of the event identifiers. For a Bloom filter \n",
    "\n",
    "In the following exercise, you will construct a simple filter similar to a quotient filter using a python dictionary for the hash table. Using this data structure consumes considerably more memory that an efficient implementation. However, using a dictionary will allow you to explore the concept of the quotient filter without the complexity of a production quality implementation.                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c11ff1",
   "metadata": {},
   "source": [
    "### Instantiate the identifier lists      \n",
    "\n",
    "To start the example, lists of customer identifiers are created. There is one list of customer identifiers and one list of identifiers of non-customers. Our goal is to constrict a hash filter that will differentiate customers from non-customers. \n",
    "\n",
    "The code in the cell below does the following:   \n",
    "1. Defines the number of hashes in the hash table (dictionary) and a set of 3 prime number hash keys. There are three hash keys and we will use three hashes for inserts, deletes and look-ups in this example.     \n",
    "2. Test that the number of hashes modulo the hash keys are all 0, to ensure the hash table will not be ragged increasing the probability of hash key collisions.     \n",
    "3. Randomly generated lists of customer and non-customer ids are created. The customer and non-customer identifiers are all integers.   \n",
    "4. The non-customer list is filtered to ensure there are no identifiers common with the customer list.  \n",
    "\n",
    "Execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_hashes = 1024\n",
    "hash_keys = [128,64,32]\n",
    "\n",
    "nr.seed(4455)\n",
    "customers = [int(8* number_of_hashes * i) for i in nr.uniform(size=50)]\n",
    "not_customers = [int(8 * number_of_hashes * i) for i in nr.uniform(size=100)]\n",
    "\n",
    "## Ensure there are no common ids between customers and non-customers\n",
    "del_list = []\n",
    "for i in range(len(not_customers)): \n",
    "    if not_customers[i] in customers: del_list.append(i)\n",
    "for i in sorted(del_list, reverse=True):\n",
    "    del not_customers[i]\n",
    "    \n",
    "## instantiate the empty hash dictionary    \n",
    "hash_dict = dict([(i,0) for i in range(number_of_hashes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d3571",
   "metadata": {},
   "source": [
    "### Define a hash function\n",
    "\n",
    "The quotient filter uses multiple hash values to reduce the probability of having The code in the cell below contains a simple modulo hash function along with some basic test cases. Execute this code and examine the results of the tests, making sure the hash keys (hash values) are unique and between 0 and `number_of_hashes`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bbbefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function(i, hash_key, modulo=number_of_hashes):\n",
    "    ## Put your code below. \n",
    "    return (i * hash_key) % modulo\n",
    "\n",
    "for test_key in hash_keys: \n",
    "    print(hash_function(55555, test_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71f06a",
   "metadata": {},
   "source": [
    "### Create and test insert and delete functions    \n",
    "\n",
    "> **Exercise 2-8:** Event identifiers must be added and deleted from the hash table (dictionary) by their hash keys. For each identifier three hashes are computed. In each hash bucket (hashed key value) an integer 1 is added to the value. To complete the code in the cell below do the following: \n",
    "> 1. Use an iterator over the set of hash keys.  \n",
    "> 2. For each key compute the hash value and use this to index a bucket in the hash table (the dictionary) and add 1 to the corresponding value.      \n",
    "> Execute your code to create the empty hash table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_event(id, hash_dictionary, keys): \n",
    "    ## Iterate over the set of keys, then for each key value\n",
    "    ## compute the hash and add 1 to the bucket in the hash table.\n",
    "    ## Place your code below\n",
    "    \n",
    "    \n",
    "    return hash_dictionary\n",
    "\n",
    "for id in customers: \n",
    "    hash_dict = insert_event(id, hash_dict, hash_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28728581",
   "metadata": {},
   "source": [
    "> With the hash table filled with the valid customer IDs it is time to examine the details of the table. Ideally, we want the buckets that the identifiers hash into to be uniformly distributed. However, this is often not the case with simple hash functions. To visualize the counts by bucket (hash value) execute the code in the cell below and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hash_keys(dictionary):\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    ax.bar(dictionary.keys(), dictionary.values());\n",
    "    ax.set_ylabel('Count of keys');\n",
    "    ax.set_xlabel('Hash value');\n",
    "    ax.set_title('Count of keys vs. hash values');\n",
    "    \n",
    "plot_hash_keys(hash_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed51cf",
   "metadata": {},
   "source": [
    "> Examine the barplot of counts of hash values in the bins. Does this distribution look close to uniform, or do you think this is a poor hash function and why?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7cf59",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935d95d",
   "metadata": {},
   "source": [
    "> There are several ways one can improve a hash function. Production quality systems use complex carefully designed hash functions. A simpler approach is to try prime numbers as the hash key. Choosing prime numbers reduces the chance of 'cyclic' behavior of the hash function which leads to lumpy hash values. To find out if this approach will help, execute the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantiate the empty hash dictionary    \n",
    "hash_dict = dict([(i,0) for i in range(number_of_hashes)])\n",
    "## Prime number hash keys\n",
    "hash_keys = [7873,5227,1009]\n",
    "for id in customers: \n",
    "    hash_dict = insert_event(id, hash_dict, hash_keys)\n",
    "    \n",
    "plot_hash_keys(hash_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924ee5d",
   "metadata": {},
   "source": [
    "> Compare the bar chart of the hash function using prime modulo factors to the one using even binary numbers. Which set of hash functions is expected to give better false positive performance and why?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c1251",
   "metadata": {},
   "source": [
    "> **Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a247f",
   "metadata": {},
   "source": [
    "An advantage of a quotient filter like approach we are using for these exercises is that one can delete a key value since a count is maintained for each bucket in the hash table. The code in the cell below includes a delete function which is applied to the customer list. The table is then scanned to see if there are any non-zero values in the buckets. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_event(id, hash_dictionary, keys): \n",
    "    for key in keys: \n",
    "        hash_dictionary[hash_function(id, key)] -= 1\n",
    "    return hash_dictionary\n",
    "\n",
    "for id in customers: \n",
    "    hash_dict = delete_event(id, hash_dict, hash_keys)\n",
    "    \n",
    "for key in hash_dict: \n",
    "    if hash_dict[key]!=0: print('Did not remove event id = ' + str(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e257e9",
   "metadata": {},
   "source": [
    "It appears that the table is empty. This fact indicates that the add and delete functions operate correctly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128023ea",
   "metadata": {},
   "source": [
    "### Apply the filter and test for false positive responses\n",
    "\n",
    "The hash filter can be used to quickly determine if it contains an identifier value. The hashes of the identifier are tested to see if they are nonzero. if any of the buckets tested contain a zero value, the identifier is not in the table. If all buckets are nonzero, there are two possibilities. Either the identifier is in the table, or this is a false positive response. Recall that no false negative is possible with this scheme. The code in the cell below will count the false positive responses to the table of non-customers.    \n",
    "\n",
    "> **Exercise 2-9:** The last component needed to complete the functionality is a look-up function to determine if an identifier is in the hash table. You will complete the code in the `is_in_hash` function in the cell below. The missing code in the function must do the following:    \n",
    "> 1. Compute a list of hash values for the identifier using the `hash_key` list.     \n",
    "> 2. Return `False` if the look-up of the value in the buckets for the hash values all contain values not equal to 0. You can test the list using the Python [all](https://docs.python.org/3/library/functions.html#all) function. If all of the values in the buckets are not 0 then return true.   \n",
    "> Execute your code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_hash(id, hash_dictionary, hash_keys):  \n",
    "    ## Put your code below\n",
    "   \n",
    "\n",
    "    else: return True  \n",
    "\n",
    "for id in customers:\n",
    "    hash_dict = insert_event(id, hash_dict, hash_keys)\n",
    "\n",
    "## Test that the     \n",
    "false_positive_count = 0 \n",
    "for id in not_customers: \n",
    "    if not is_in_hash(id, hash_dict, hash_keys): false_positive_count += 1\n",
    "print('Number of false positives = ' + str(false_positive_count))                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49deeb",
   "metadata": {},
   "source": [
    "> For a perfect (uniform) hash function the theoretical false positive rate can be computed by the following relationship:  > $$P(false\\ positive) = \\Big[1 - exp \\big(\\frac{- k n}{m} \\big) \\Big]^k$$    \n",
    "> Where, \n",
    "> - $k = $ number of hash functions.    \n",
    "> - $m = $ length of the hash table.   \n",
    "> - $n = $ number of identifiers in the hash table.   \n",
    "> \n",
    "> In the cell below compute the theoretical probability a false positive for this exercise. Then multiply by the number of non-customers tested to find the expected number of false positives.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "prob_of_false_positive = (1 - math.exp(-3*50/number_of_hashes))**3\n",
    "prob_of_false_positive * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e8d74",
   "metadata": {},
   "source": [
    "> Compare the actual number of false positives to the expected number of false positives. Explain the difference.   \n",
    "> **End of exercise:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8c46b",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422c1ff",
   "metadata": {},
   "source": [
    "> **Exercise 2-10:** There is one last bit of analysis that should be done. We have been using a somewhat arbitrary number of hash functions. But, it is possible to compute an optimal number of hash functions for an ideal situation using the following relationship, using the same notation used in Exercise 2-9:     \n",
    "> $$k_{optimal} = \\frac{m}{n} \\big( log(2) \\big)^2$$      \n",
    ">      \n",
    "> For $n = [50, 100, 200, 400]$ compute and print the optimal number of hashes, $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726920cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f6eb9",
   "metadata": {},
   "source": [
    "> It is clear that for ideal hash functions we have been working with too few functions. However, there are several reasons why this ideal situation is not achieved. Consider two primary reasons that a smaller number of hash functions that the theoretical number might be better in a case like this, which include the clustering of the hash values and the fact that more than 50 identifiers in the hash table. State the reasons you can think of for using fewer hash functions.    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae543b88",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f868cca",
   "metadata": {},
   "source": [
    "#### Copyright, 2021, 2022, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
