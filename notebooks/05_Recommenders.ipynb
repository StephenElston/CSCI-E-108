{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c61735ea",
      "metadata": {
        "id": "c61735ea"
      },
      "source": [
        "# Assignment 05\n",
        "\n",
        "## Recommender Systems\n",
        "\n",
        "## CSCI E-108\n",
        "\n",
        "### Luciano Carvalho"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775d22f6",
      "metadata": {
        "id": "775d22f6"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Recommender algorithms are widely used in commerce. Further, the basic algorithms have found applications in other areas. In this Assignment you gain some experience working with recommender data, the collaborative filtering algorithm and some versions of the nonnegative matrix factorization algorithm.           \n",
        "\n",
        "This Assignment uses the Python [Surprise](https://surprise.readthedocs.io/en/stable/index.html) package a Scikit for recommender system experimentation. This package is in maintenance mode, but is still useful for learning recommender concepts. If you have not previously installed Surprise you will need to install it. Generally, the best way to install this older package is to run the command below from a command prompt on your system. If you have difficulty, you can refer to [this Stackoverflow thread](https://stackoverflow.com/questions/62042317/cannot-install-scikit-surprise-on-my-jupyter-notebook).      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: install scikit-surprise package\n",
        "\n",
        "!pip install scikit-surprise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKZlgS5xqjqR",
        "outputId": "81862a85-2030-418b-9266-5240979d4821"
      },
      "id": "CKZlgS5xqjqR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/154.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357250 sha256=45359b49ebb0ffb13ed46ad0369815fd511d11b34b1933c08fa9f315cc31f679\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "raw",
      "id": "eb6b84db",
      "metadata": {
        "id": "eb6b84db"
      },
      "source": [
        "conda install -c conda-forge scikit-surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1794d61",
      "metadata": {
        "id": "a1794d61"
      },
      "source": [
        "> **Computational note:** The Surprise package was never intended as a production recommender system. If you want to explore options for production scalable recommender systems some options include:   \n",
        "> 1. The Neo4J graph database contains extensive support for enterprise recommendation systems. You can see a tutorial introduction [here](https://neo4j.com/developer/cypher/guide-build-a-recommendation-engine/).   \n",
        "> 2. The major cloud machine learning platforms, such as Keras and PyTorch, have increasing massive scale support for recommendation algorithms. You can find some getting started information for both [Kera](https://keras.io/examples/structured_data/collaborative_filtering_movielens/) and [PyTorch](https://pytorch.org/blog/introducing-torchrec/).\n",
        "\n",
        "Execute the code in the cell below to import the packages you will need for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "56c703c5",
      "metadata": {
        "id": "56c703c5"
      },
      "outputs": [],
      "source": [
        "from surprise.prediction_algorithms.matrix_factorization import SVD, NMF\n",
        "from surprise.prediction_algorithms.baseline_only import BaselineOnly\n",
        "from surprise.prediction_algorithms.knns import KNNBasic\n",
        "from surprise import Dataset\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "187e0e31",
      "metadata": {
        "id": "187e0e31"
      },
      "source": [
        "## Load and Explore the Dataset   \n",
        "\n",
        "In this Assignment you will work with the MovieLens 100k dataset. This dataset includes a total of 100,000 movie ratings from IMDB.\n",
        "\n",
        "Before working with recommender algorithms, you will explore these data. Exploration of data is an essential step in any data mining process.     \n",
        "\n",
        "The code in the cell below loads the dataset and splits it into training and testing datasets. Execute this code. If you are asked if you want to download the data you must answer y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13593f0",
      "metadata": {
        "id": "b13593f0"
      },
      "outputs": [],
      "source": [
        "## Load the movielens-100k dataset\n",
        "## Answer y if you are asked to download the dataset\n",
        "movie_lense_data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "## Split the dataset into training and test subsets.\n",
        "np.random.seed(4517)\n",
        "ml_trainset, ml_testset = train_test_split(movie_lense_data, test_size=0.20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d69e4e",
      "metadata": {
        "id": "56d69e4e"
      },
      "source": [
        "Before experimenting with the recommender algorithms we will explore the data. As a first step of the execute the code in the cell below to print some characteristics of the training dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a87679",
      "metadata": {
        "id": "78a87679"
      },
      "outputs": [],
      "source": [
        "print('For training dataset:')\n",
        "print(\"Number of users = {}\".format(ml_trainset.n_users))\n",
        "print(\"Number of items = {}\".format(ml_trainset.n_items))\n",
        "print(\"Number of ratings = {}\".format(ml_trainset.n_ratings))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1dd669a",
      "metadata": {
        "id": "b1dd669a"
      },
      "source": [
        "You can see some basic characteristics of the training dataset. If these data were represented as a rectangular table or matrix they would require, over 1.5 million, and at least that many bytes. In practice, a more efficient representation is used.\n",
        "\n",
        "The Surprise package, like all real-world recommender software, uses a space efficient representation. The `ur` attribute of the Surprise data object contains a dictionary with the recommender data. The keys of the dictionary are numeric identifiers of the users. The values of this key-value pairs are a list of tuples. The tuples are key-value pairs, with the key being the item identifier and the rating. In summary, there is a hashed user key to item key-value pairs. This nesting of key-value pairs is necessary since Python dictionaries only support a single hashed key per value.        \n",
        "\n",
        "The code in the cell below prints the list of key-value pairs for a single user key. There are multiple item identifier, rating, tupples for user 5. Execute this code and examine the results.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42fa930b",
      "metadata": {
        "id": "42fa930b"
      },
      "outputs": [],
      "source": [
        "## Print the product-rating tupples in the 'ur' attribute of the data object from user 6\n",
        "print(f\"The number of recommendations = {len(ml_trainset.ur[5])}\")\n",
        "ml_trainset.ur[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d79af1c7",
      "metadata": {
        "id": "d79af1c7"
      },
      "source": [
        "User 5 has only rated 32 items of 1647 total items.     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f28953",
      "metadata": {
        "id": "79f28953"
      },
      "source": [
        "> **Exercise 05-1:** To gain some understanding of the nature of the ratings, in the cell below create and execute code to compute and print two summary statistics in decimal fraction format. Use the attributes of the training dataset to extract the required parameters. See the foregoing example for an example of accessing the attributes of the data object.        \n",
        "> 1. Fraction of all items rated by user 5.\n",
        "> 2. The fraction of possible item-user pairs in the utility matrix that have non-null values.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6870b0d",
      "metadata": {
        "id": "e6870b0d"
      },
      "outputs": [],
      "source": [
        "## Put your code here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7ae678",
      "metadata": {
        "id": "2e7ae678"
      },
      "source": [
        "> Q1. Given that real-world online catalog (products, videos, music, etc.) can contain millions of items, do you think these fractions for an individual user and the average user are realistic or too high?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15fea552",
      "metadata": {
        "id": "15fea552"
      },
      "source": [
        "> **Answer:**     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b70a642",
      "metadata": {
        "id": "8b70a642"
      },
      "source": [
        "> 3. Create two lists with the following values, a) list named `mean_ratings` containing the mean rating of each user, and b) a list named `number_ratings` that contains the count of ratings for each user. The user ratings are represented with a Python dictionary, with the user identifier as key, in the `ur` attribute of the training dataset. You can extract the keys (user identifiers) with the `keys` method.  \n",
        "> 4. Once you have completed your code, execute the code in the cell below to display the histograms.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4639c3f9",
      "metadata": {
        "id": "4639c3f9"
      },
      "outputs": [],
      "source": [
        "## Compute the mean rating for each of the users in the training dataset\n",
        "## Put your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Create a histograms of the results\n",
        "fig,ax= plt.subplots(1,2,figsize=(15,4))\n",
        "_=ax[0].hist(mean_ratings, bins=20, alpha=0.5)\n",
        "_=ax[0].set_xlabel('Mean user rating')\n",
        "_=ax[0].set_ylabel('Frequency')\n",
        "_=ax[0].set_title('Distribution of mean user ratings')\n",
        "_=ax[1].hist(number_ratings, bins=20, alpha=0.5)\n",
        "_=ax[1].set_xlabel('Number of items rated')\n",
        "_=ax[1].set_ylabel('Frequency')\n",
        "_=ax[1].set_title('Distribution of number of items rated by user')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b23fd56",
      "metadata": {
        "id": "1b23fd56"
      },
      "source": [
        "> Provide short answers to the following questions:   \n",
        "> Q2. Is the distribution of the mean user ratings a bit skewed? What does this skew tell us?   \n",
        "> Q3. Notice the rapid decay in the frequency of items rated by users. What does this long tail tell you about user behavior?  \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671d7810",
      "metadata": {
        "id": "671d7810"
      },
      "source": [
        "> **Answers:**   \n",
        "> Q2.        \n",
        "> Q3.     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cceab1c",
      "metadata": {
        "id": "4cceab1c"
      },
      "source": [
        "## Baseline Only Model\n",
        "\n",
        "One possible answer to the cold start problem is to use a **baseline only** model. In this model we just use the average or baseline values for users or items to predict ratings. This same approach can be applied to users who consume an item, but provide no rating."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3cc000",
      "metadata": {
        "id": "0a3cc000"
      },
      "source": [
        "> **Exercise 05-2:** You will now create and evaluate a baseline recommender model. Do the following:   \n",
        "> 1. Instantiate a [surprise.prediction_algorithms.baseline_only.BaselineOnly](https://surprise.readthedocs.io/en/stable/basic_algorithms.html) model object and it with the `fit` method using the `ml_trainset` as the argument. Name your model object `baseline_model`.  \n",
        "> 2. Compute the recommender predictions with the `test` method using the `ml_testset` as the argument.   \n",
        "> 3. Compute and print the RMSE using [surprise.accuracy.rmse](https://surprise.readthedocs.io/en/stable/accuracy.html) and MAE using [surprise.accuracy.mae](https://surprise.readthedocs.io/en/stable/accuracy.html) with the predictions as the argument in both cases.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d92d5c6c",
      "metadata": {
        "id": "d92d5c6c"
      },
      "outputs": [],
      "source": [
        "np.random.seed(864)\n",
        "## Instantiate the model object and fit to the training data\n",
        "# Put your code below\n",
        "\n",
        "\n",
        "\n",
        "## Compute the predicitons from the model\n",
        "# Put your code below\n",
        "\n",
        "\n",
        "# Compute some model performance statistics\n",
        "# Put your code below\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a9ad54e",
      "metadata": {
        "id": "3a9ad54e"
      },
      "source": [
        "> Q1. Does the accuracy performance of this model seem good given its simplicity?   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a523a8d3",
      "metadata": {
        "id": "a523a8d3"
      },
      "source": [
        "> **Answer:**    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "238cf7fb",
      "metadata": {
        "id": "238cf7fb"
      },
      "source": [
        "> 4. Execute the code in the cell below to display histograms of the item and user biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d87746",
      "metadata": {
        "id": "e4d87746"
      },
      "outputs": [],
      "source": [
        "## plot the biases\n",
        "fig,ax= plt.subplots(1,2,figsize=(15,4))\n",
        "_=ax[0].hist(baseline_model.bi, bins=20, alpha=0.5)\n",
        "_=ax[0].set_xlabel('Item bias')\n",
        "_=ax[0].set_ylabel('Frequency')\n",
        "_=ax[0].set_title('Distribution of item bias')\n",
        "_=ax[0].set_xlim(-1.5,1.5)\n",
        "_=ax[1].hist(baseline_model.bu, bins=20, alpha=0.5)\n",
        "_=ax[1].set_xlabel('User bias')\n",
        "_=ax[1].set_ylabel('Frequency')\n",
        "_=ax[1].set_title('Distribution of user bias')\n",
        "_=ax[1].set_xlim(-1.5,1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820d4896",
      "metadata": {
        "id": "820d4896"
      },
      "source": [
        "> Q2. These bias values are approximately Normally distributed, but with somewhat heavy tails. Examine the range of these biases. What does the fact that these ranges are small compared to the range of ratings tell you about the magnitude of the bias adjustments?      "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb87ece",
      "metadata": {
        "id": "9bb87ece"
      },
      "source": [
        "> **Answer:**     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289257c3",
      "metadata": {
        "id": "289257c3"
      },
      "source": [
        "> 5. Ideally, the biases should be uniformly distributed with identifier. In other words, statistically independent of the identifiers. In other words, **homoskedastic**. Execute the code to display the relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea4f961",
      "metadata": {
        "id": "2ea4f961"
      },
      "outputs": [],
      "source": [
        "## plot the biases\n",
        "fig,ax= plt.subplots(1,2,figsize=(15,4))\n",
        "_=ax[0].scatter(range(len(baseline_model.bi)), baseline_model.bi)\n",
        "_=ax[0].set_xlabel('Item identifier')\n",
        "_=ax[0].set_ylabel('Bias value')\n",
        "_=ax[0].set_title('Item bias vs. item identifier')\n",
        "_=ax[1].scatter(range(len(baseline_model.bu)), baseline_model.bu)\n",
        "_=ax[1].set_xlabel('User identifier')\n",
        "_=ax[1].set_ylabel('Bias value')\n",
        "_=ax[1].set_title('User bias vs. user identifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b12326c3",
      "metadata": {
        "id": "b12326c3"
      },
      "source": [
        "> Q3. Does the distribution of the biases look largely uniform with item identifier or not?    \n",
        "> Q4. Does the distribution of the biases look largely uniform with user identifier or not?   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f781f6d",
      "metadata": {
        "id": "9f781f6d"
      },
      "source": [
        "> **Answers:**   \n",
        "> Q3.       \n",
        "> Q4.     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f394b431",
      "metadata": {
        "id": "f394b431"
      },
      "source": [
        "## Collaborative Filtering Model  \n",
        "\n",
        "The collaborative filtering model is an unsupervised learning model using similarity item-item or user-user similarity measures. Items with the highest similarity measures are recommended.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1cbee48",
      "metadata": {
        "id": "f1cbee48"
      },
      "source": [
        "> **Exercise 05-3:** You will now create and evaluate a baseline recommender model. Do the following:   \n",
        "> 1. Instantiate a [surprise.prediction_algorithms.knns.KNNBasic](https://surprise.readthedocs.io/en/stable/knn_inspired.html) model object and it with the `fit` method using the `ml_trainset` as the argument. Name your model object `collaborative_filter_model`.  \n",
        "> 2. Compute the recommender predictions with the `test` method using the `ml_testset` as the argument.   \n",
        "> 3. Compute and print the RMSE using `surprise.accuracy.rmse and MAE using `surprise.accuracy.mae` with the predictions as the argument in both cases.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472e0513",
      "metadata": {
        "id": "472e0513"
      },
      "outputs": [],
      "source": [
        "np.random.seed(258)\n",
        "## Instantiate the model object and fit to the training data\n",
        "## Put your code here\n",
        "\n",
        "\n",
        "\n",
        "## Compute the predicitons from the model\n",
        "## Put your code here\n",
        "\n",
        "\n",
        "# Compute some model performance statistics\n",
        "## Put your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a130312",
      "metadata": {
        "id": "0a130312"
      },
      "source": [
        "> Provide short answers to the following questions:     \n",
        "> Q1. Compare the performance of this model to the baseline model. Is it better or worse?     \n",
        "> Q2. Keeping in mind that the rating sampling is more dense that a real-world example, what does this tell you about the effectiveness of a baseline model vs. the model based on similarity.    \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de3d8f1",
      "metadata": {
        "id": "1de3d8f1"
      },
      "source": [
        "> **Answers:**       \n",
        "> Q1.           \n",
        "> Q2.     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b69bfe",
      "metadata": {
        "id": "39b69bfe"
      },
      "source": [
        "> 4. The distribution of the similarities gives insight into the behavior of the model. Execute the code in the to display the histogram of the user-user similarity measures.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf7db96b",
      "metadata": {
        "id": "bf7db96b"
      },
      "outputs": [],
      "source": [
        "_=plt.hist(collaborative_filter_model.sim.flatten(), bins=40, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34716d92",
      "metadata": {
        "id": "34716d92"
      },
      "source": [
        "> Q3. Examine the plot. Notice there are a number of similarities which are or are close to 0 or 1, and another spike around 0.5. Is the distribution of these similarities skewed? If so, what does this skew tell you about how strong a predictor similarity can be in this case.    \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f4c0ae",
      "metadata": {
        "id": "b2f4c0ae"
      },
      "source": [
        "> **Answer:**           "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f7c1c1",
      "metadata": {
        "id": "49f7c1c1"
      },
      "source": [
        "## Matrix Factorization Methods   \n",
        "\n",
        "Matrix factorization algorithms are generally considered to be the state of the art for recommenders. The matrix factorization methods find a set of latent (non-observable) factor variable values, which are used to compute rating estimates. Additionally, user and item rating bias adjustments are applied.    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52747377",
      "metadata": {
        "id": "52747377"
      },
      "source": [
        "> **Exercise 05-4:** You will now create and evaluate a baseline recommender model. Do the following:   \n",
        "> 1. Instantiate a [surprise.prediction_algorithms.matrix_factorization.SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) model object, using 10 factors, and it with the `fit` method using the `ml_trainset` as the argument. Name your model object `svd_model`.  \n",
        "> 2. Compute the recommender predictions with the `test` method using the `ml_testset` as the argument.   \n",
        "> 3. Compute and print the RMSE using `surprise.accuracy.rmse` and MAE using `surprise.accuracy.mae` with the predictions as the argument in both cases.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5acf53",
      "metadata": {
        "id": "2c5acf53"
      },
      "outputs": [],
      "source": [
        "np.random.seed(9741)\n",
        "## Instantiate the model object and fit to the training data\n",
        "## Put your code here\n",
        "\n",
        "\n",
        "\n",
        "## Compute the predicitons from the model\n",
        "## Put your code here\n",
        "\n",
        "\n",
        "# Compute some model performance statistics\n",
        "## Put your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76cd2919",
      "metadata": {
        "id": "76cd2919"
      },
      "source": [
        "> Provide short answers to the following questions:   \n",
        "> Q1. Compare the performance of this model to the baseline model. Is it better or worse?     \n",
        "> Q2. Keeping in mind that the rating sampling is more dense that a real-world example, what does this tell you about the effectiveness of a baseline model vs. the model based on latent factors.    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b22b947",
      "metadata": {
        "id": "3b22b947"
      },
      "source": [
        "> **Answers:**       \n",
        "> Q1.        \n",
        "> Q2.    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eeb2ef1",
      "metadata": {
        "id": "8eeb2ef1"
      },
      "source": [
        "> 4. The distribution of the item and factor values gives insight into the behavior of the model. Execute the code in the to display the histograms of the factors for items and users.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bd4f30",
      "metadata": {
        "id": "b6bd4f30"
      },
      "outputs": [],
      "source": [
        "print('Shape of item factor matrix: {}'.format(svd_model.qi.shape))\n",
        "print('Shape of user factor matrix: {}'.format(svd_model.pu.shape))\n",
        "\n",
        "## Create a histograms of the factor values\n",
        "fig,ax= plt.subplots(1,2,figsize=(15,4))\n",
        "_=ax[0].hist(svd_model.qi.flatten(), bins=20, alpha=0.5)\n",
        "_=ax[0].set_xlabel('Item factor values')\n",
        "_=ax[0].set_ylabel('Frequency')\n",
        "_=ax[0].set_title('Distribution of item factor values')\n",
        "_=ax[0].set_xlim(-.75,.75)\n",
        "_=ax[1].hist(svd_model.pu.flatten(), bins=20, alpha=0.5)\n",
        "_=ax[1].set_xlabel('User factor values')\n",
        "_=ax[1].set_ylabel('Frequency')\n",
        "_=ax[1].set_title('Distribution of user factor values')\n",
        "_=ax[1].set_xlim(-.75,.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0342c0",
      "metadata": {
        "id": "fe0342c0"
      },
      "source": [
        "> 5. Finally, you will compute the compression of the factor model compared to a model using the full utility matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4efb4e0",
      "metadata": {
        "id": "e4efb4e0"
      },
      "outputs": [],
      "source": [
        "## Calculation of compression of factor model\n",
        "n_reviews = 0\n",
        "for key in ml_trainset.ur.keys():\n",
        "    n_reviews += len(ml_trainset.ur[key])\n",
        "print(f\"Total number of reviews = {n_reviews}\")\n",
        "utility_bytes = 8*n_reviews + 4*ml_trainset.n_users\n",
        "print(f\"Bytes for all review tuples = {utility_bytes}\")\n",
        "\n",
        "P_size = n_factors * ml_trainset.n_users\n",
        "Q_size = n_factors * ml_trainset.n_items\n",
        "print(f\"Values in P = {P_size}\")\n",
        "print(f\"Values in Q = {Q_size}\")\n",
        "PQ_bytes = 4 * (P_size + Q_size)\n",
        "print(f\"Total bytes for P + Q = {PQ_bytes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e046cf72",
      "metadata": {
        "id": "e046cf72"
      },
      "source": [
        "> Provide short answers to the following questions:     \n",
        "> Q3. Examine the distribution of the factor values. Compare these values to the distribution of baseline or bias values for the Baseline model. What do the differences in the range of values tell you about the effect of subtracting the bias terms before computing the factor values?     \n",
        "> Q4. Notice the dimensions of factor matrices computed, Q, P. Is this representation compact compared to the original data set, and why?     \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58397961",
      "metadata": {
        "id": "58397961"
      },
      "source": [
        "> **Answers:**   \n",
        "> Q3.     \n",
        "> Q4.    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cfda811",
      "metadata": {
        "id": "2cfda811"
      },
      "source": [
        "#### Copyright 2021, 2022, 2023, Stephen F Elston. All rights reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557b7b3d",
      "metadata": {
        "id": "557b7b3d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}