{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "967b4d75",
      "metadata": {
        "id": "967b4d75"
      },
      "source": [
        "# Assignment 04\n",
        "## Web Search\n",
        "## CSCI E-108\n",
        "### Luciano Carvalho\n",
        "\n",
        "> **Instructions:** For this assignment you will complete the exercises shown. Most exercises involve creating and executing some Python code. Additionally, most exercises have questions for you to answer. You can answer questions by creating a Markdown cell and writing your answer. If you are not familiar with Markdown, you can find a brief tutorial [here](https://www.markdownguide.org/cheat-sheet/).     \n",
        "\n",
        "In this assignment you will gain some experience and insight into how web search algorithms work. Specifically, you will implement versions of three algorithms, simple PageRank, damped PageRank, and the HITS algorithm. All three of these algorithms use a **directed graph model** of the web.   \n",
        "\n",
        "The small data examples and coding methods used here are not directly scalable to web sized problems. Rather, the point is for you to understand the basic characteristics of these web search algorithms. Web scale searching requires massive resources not readily available to most people."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4218530",
      "metadata": {
        "id": "c4218530"
      },
      "source": [
        "## Simple PageRank Example\n",
        "\n",
        "To get a feeling for the basics of the PageRank algorithm you will create and test simple code.\n",
        "\n",
        "As a first step, execute the code in the cell below to import the packages you will need for the exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "60a1d748",
      "metadata": {
        "id": "60a1d748"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "320e8bbe",
      "metadata": {
        "id": "320e8bbe"
      },
      "source": [
        "### Directed Graph of Web Pages\n",
        "\n",
        "We will start with a simple example. Figure 1 shows a set of web pages and their hyperlinks. This is a **directed graph** with the **pages as nodes** and the **hyperlinks as the directed edges**. This graph is **complete**. Every page is accessible from any other page, possibly with visits to intermediate nodes required.  \n",
        "\n",
        "<img src=\"../images/Web1.png\" alt=\"Drawing\" style=\"width:500px; height:400px\"/>\n",
        "<center>Figure 1: A small set of web pages</center>\n",
        "\n",
        "The directed edges of the graph define the association between the nodes. For the **association matrix**, a directed edge, or hyperlink, runs from a node's column to the terminal node's row. The association is binary. The directed edge either exists or it does not."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1a3f1b",
      "metadata": {
        "id": "af1a3f1b"
      },
      "source": [
        "> **Exercise 04-1:** In the cell below you will create the association matrix and the initial page probability vector. Do the following:  \n",
        "> 1. Create the association matrix, $A$, using [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html). This matrix is constructed with a 1 where a page in a column has a directed edge linking to another page in a row, and 0 elsewhere. In matrix notation, the element $a_{i,j}$ indicates the presence or absence of a directed edge from node $n_j$ to node $n_i$.   \n",
        "> 2. Print the shape of your association matrix as a check.\n",
        "> 3. Print the in degree and out degree of each node in your association matrix, using [numpy.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html). Set the argument `axis` to 1 to sum across rows and 0 to sum down columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "78bcdbf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78bcdbf0",
        "outputId": "ccf06b49-e289-4d9a-ca04-783a87c98564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the association matrix: (5, 5)\n",
            "In-Degree of each node: [2 2 2 3 2]\n",
            "Out-Degree of each node: [3 3 1 2 2]\n"
          ]
        }
      ],
      "source": [
        "# Association matrix based on the provided graph\n",
        "A = np.array([\n",
        "    [0, 1, 1, 1, 0],  # Page 1\n",
        "    [1, 0, 0, 1, 1],  # Page 2\n",
        "    [0, 0, 0, 1, 0],  # Page 3\n",
        "    [0, 1, 0, 0, 1],  # Page 4\n",
        "    [1, 0, 1, 0, 0]   # Page 5\n",
        "])\n",
        "\n",
        "# Print the shape of the association matrix\n",
        "print(\"Shape of the association matrix:\", A.shape)\n",
        "\n",
        "# Calculate the in-degree and out-degree of each node\n",
        "in_degree = np.sum(A, axis=0)\n",
        "out_degree = np.sum(A, axis=1)\n",
        "\n",
        "print(\"In-Degree of each node:\", in_degree)\n",
        "print(\"Out-Degree of each node:\", out_degree)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4723ad6b",
      "metadata": {
        "id": "4723ad6b"
      },
      "source": [
        "> Are the out degree and in degree you computed from the association matrix consistent with the graph in Figure 1?    \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb63780",
      "metadata": {
        "id": "edb63780"
      },
      "source": [
        "> **Answer:** Yes, the out-degree and in-degree values I computed from the association matrix are consistent with the graph in Figure 1. The in-degrees and out-degrees match the links shown in the graph perfectly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1fc5fd8",
      "metadata": {
        "id": "e1fc5fd8"
      },
      "source": [
        "### Apply Simple Page Rank\n",
        "\n",
        "The normalized transition probability matrix, $M$, is then computed from the association matrix, $A$:\n",
        "\n",
        "$$M = A D^{-1}$$\n",
        "\n",
        "Where, $D^{-1}$ is the inverse of a matrix with the out degree values on the diagonal and zeros elsewhere.  \n",
        "\n",
        "You can see from the foregoing that $M$ distributes the influence of the page by the in|verse of the out degree. In other words, the influence is inversely weighted by the number of pages each page links to."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd99dfa",
      "metadata": {
        "id": "2cd99dfa"
      },
      "source": [
        "> **Exercise 04-2:** You will now compute the normalized transition matrix, $M$. To do so create a function called `norm_association` with the association matrix as the argument. Do the following:\n",
        "> 1. Create your function `norm_association` which will do the following:  \n",
        ">    - Compute the sum of the columns of the association matrix using `numpy.sum` with the `axis=0` argument to sum along columns.\n",
        ">   - Compute the inverse of the column sums as a vector. Be sure to avoid zero divides, which will occur in subsequent exercises. Use the `where` argument of [numpy.divide](https://numpy.org/doc/stable/reference/generated/numpy.divide.html) to do so. If the column sum is 0 the inverse is set to 0.0.   \n",
        ">   - Create a square diagonal matrix from the inverse column sums using [numpy.diag](https://numpy.org/doc/stable/reference/generated/numpy.diag.html) to form the inverse out degree diagonal matrix.\n",
        ">  - Finally, return the matrix product of the association matrix and the (diagonal) inverse out degree matrix using [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).  \n",
        "> 2. Save and print the normalized transition matrix.  \n",
        "> 3. Compute and print the column sums of the normalized transition matrix to ensure they all add to 1.0.\n",
        "> Execute the code you have created and examine the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5f24b853",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f24b853",
        "outputId": "08e0b379-7abb-4e35-d455-34e565774830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized transition matrix:\n",
            " [[0.         0.5        0.5        0.33333333 0.        ]\n",
            " [0.5        0.         0.         0.33333333 0.5       ]\n",
            " [0.         0.         0.         0.33333333 0.        ]\n",
            " [0.         0.5        0.         0.         0.5       ]\n",
            " [0.5        0.         0.5        0.         0.        ]]\n",
            "Column sums of the normalized transition matrix: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "def norm_association(A):\n",
        "    '''Function to normalize the association matrix by out degree.\n",
        "    The function accounts for cases where the column sum is 0'''\n",
        "\n",
        "    # 1.1 Compute the sum of the columns of the association matrix\n",
        "    col_sums = np.sum(A, axis=0)\n",
        "\n",
        "    # 1.2 Compute the inverse of the column sums, avoiding divide by zero\n",
        "    inv_col_sums = np.divide(1, col_sums, where=col_sums!=0, out=np.zeros_like(col_sums, dtype=float))\n",
        "\n",
        "    # 1.3 Create a square diagonal matrix from the inverse column sums\n",
        "    D_inv = np.diag(inv_col_sums)\n",
        "\n",
        "    # 1.4 Return the matrix product of the association matrix and the inverse out degree matrix\n",
        "    M = np.matmul(A, D_inv)\n",
        "\n",
        "    return M\n",
        "\n",
        "# Code to execute the function and check the column sums\n",
        "\n",
        "# Define the association matrix A\n",
        "A = np.array([\n",
        "    [0, 1, 1, 1, 0],  # Page 1\n",
        "    [1, 0, 0, 1, 1],  # Page 2\n",
        "    [0, 0, 0, 1, 0],  # Page 3\n",
        "    [0, 1, 0, 0, 1],  # Page 4\n",
        "    [1, 0, 1, 0, 0]   # Page 5\n",
        "])\n",
        "\n",
        "# 2. Save and Print the association matrix\n",
        "M = norm_association(A)\n",
        "print(\"Normalized transition matrix:\\n\", M)\n",
        "\n",
        "# 3. Compute and print the column sums of the normalized transition matrix\n",
        "col_sums_M = np.sum(M, axis=0)\n",
        "print(\"Column sums of the normalized transition matrix:\", col_sums_M)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226c5d12",
      "metadata": {
        "id": "226c5d12"
      },
      "source": [
        "> Provide short answers to the following questions:     \n",
        "> 1. Do the number of non-zero values in each column match the out degree for the corresponding node?     \n",
        "> 2. Are all the column sums 1.0, and why is this required for a transition probability matrix.    \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3fb131",
      "metadata": {
        "id": "7f3fb131"
      },
      "source": [
        "> **Answers:**\n",
        "\n",
        "> 1. Yes, the number of non-zero values in each column matches the out degree for the corresponding node.\n",
        "\n",
        "> 2. Yes, all the column sums are 1.0. This is required because in a transition probability matrix, each column represents the distribution of probability from one node to all other nodes. The sum of probabilities for each node must equal 1 to ensure that the total probability is conserved and properly distributed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c15a3d18",
      "metadata": {
        "id": "c15a3d18"
      },
      "source": [
        "### Computing the Simple Page Rank\n",
        "\n",
        "With the transition probability matrix, $M$ computed it is time to investigate the convergence of the PageRank algorithm. You can think of the PageRank algorithm as a series of transitions of a Markov Chain. Given the transition probability matrix, $M$, the update, or single Markov transition, of the page probabilities, $p_i$, is computed:\n",
        "\n",
        "$$p_i = M p_{i-1}$$\n",
        "\n",
        "The Markov chain can be executed for a great many transitions. The result of $n$ transitions, starting from an initial set of page probabilities, $p_0$, can be written:  \n",
        "\n",
        "$$p_n = M^n p_{0}$$\n",
        "\n",
        "At convergence the page probabilities, $p_n$, approach a constant or **steady state** value. This steady state probability vector values are the PageRank of the web pages.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da47d2b",
      "metadata": {
        "id": "7da47d2b"
      },
      "source": [
        "> **Exercise 04-3:** You will now create and execute code with the goal of getting a feel for how the page probabilities change for a single transition of a Markov process. The accomplish this task you will create a function called `transition` with arguments of the the normalized transition probability matrix and the vector of page probabilities. Specifically you will:   \n",
        "> 1. Complete the function `transition` which uses [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) to compute the product of the transition matrix and the page probability vector.  \n",
        "> 2. Define a state vector p, with uniform initial probabiltes. Print this initial statevector.\n",
        "> 3. Execute the `transition` function on the normalized transition probability matrix and vector of initial page probabilities you have created, saving the result to a new variable name. Print the result.\n",
        "> 4. Print the Euclidean (L2) norm of the difference between the initial page probabilities and the updated page probabilities.\n",
        "> 5. Print the sum of the page probabilities computed with `transition`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "83e71abd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83e71abd",
        "outputId": "b58dd837-b1fd-49a0-9609-ff4f8936db8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state vector p0: [0.2 0.2 0.2 0.2 0.2]\n",
            "Updated state vector p1: [0.26666667 0.26666667 0.06666667 0.2        0.2       ]\n",
            "Euclidean (L2) norm of the difference: 0.16329931618554527\n",
            "Sum of the page probabilities: 1.0\n"
          ]
        }
      ],
      "source": [
        "def transition(transition_probs, probs):\n",
        "    '''Function to compute the probabilities resulting from a\n",
        "    single transition of a Markov process'''\n",
        "\n",
        "    # 1. compute the product of the transition matrix and the page prob vector\n",
        "    return np.dot(transition_probs, probs)\n",
        "\n",
        "# Compute probabilities after first state transition and print the summaries\n",
        "\n",
        "# Define the association matrix A\n",
        "A = np.array([\n",
        "    [0, 1, 1, 1, 0],  # Page 1\n",
        "    [1, 0, 0, 1, 1],  # Page 2\n",
        "    [0, 0, 0, 1, 0],  # Page 3\n",
        "    [0, 1, 0, 0, 1],  # Page 4\n",
        "    [1, 0, 1, 0, 0]   # Page 5\n",
        "])\n",
        "\n",
        "# Normalize the association matrix\n",
        "M = norm_association(A)\n",
        "\n",
        "# 2. Define a state vector p, with uniform initial probabilities\n",
        "p0 = np.full(M.shape[0], 1.0 / M.shape[0])\n",
        "print(\"Initial state vector p0:\", p0)\n",
        "\n",
        "# 3. Execute the transition function\n",
        "p1 = transition(M, p0)\n",
        "print(\"Updated state vector p1:\", p1)\n",
        "\n",
        "# 4. Print the Euclidean (L2) norm of the difference\n",
        "l2_norm_diff = np.linalg.norm(p1 - p0)\n",
        "print(\"Euclidean (L2) norm of the difference:\", l2_norm_diff)\n",
        "\n",
        "# 5. Print the sum of the page probabilities computed with transition\n",
        "sum_p1 = np.sum(p1)\n",
        "print(\"Sum of the page probabilities:\", sum_p1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63564579",
      "metadata": {
        "id": "63564579"
      },
      "source": [
        "> Provide short answers to the following questions:   \n",
        "> 1. Is the sum of the page probabilities equal to 1.0 as it should be?       \n",
        "> 2. Considering in degree of the pages, are the relative changes in the page probabilities what you would expect and why?    \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8594649a",
      "metadata": {
        "id": "8594649a"
      },
      "source": [
        "> **Answers:**\n",
        "\n",
        "> 1. Yes, the sum of the page probabilities is equal to 1.0, which is expected for a transition probability matrix\n",
        "\n",
        "> 2. Yes, the relative changes in the page probabilities are what I would expect. Pages with higher in-degrees, like Page 1 and Page 2, see an increase in their probabilities because they receive more links, indicating they are more important or influential in the network. Pages with lower in-degrees, like Page 3, have a lower probability, reflecting their lower influence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3434f366",
      "metadata": {
        "id": "3434f366"
      },
      "source": [
        "> **Exercise 04-4:** You will continue with computing transitions of the Markov chain. Use the `transition` function with the normalized transition probability matrix and the page probability vector computed from the first transition as arguments. Your code must do the following:  \n",
        "> 1. Compute and print the resulting page probabilities of the second transition.\n",
        "> 2. Compute and print the Euclidean (L2) norm of the difference between the page probabilities before and after the transition.\n",
        "> 3. Compute and print the sum of the page probabilities.\n",
        "> 4. Display the page probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ed8d76cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed8d76cb",
        "outputId": "a6454768-eedf-4586-f8db-a47f107945f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated state vector p2: [0.23333333 0.3        0.06666667 0.23333333 0.16666667]\n",
            "Euclidean (L2) norm of the difference (second transition): 0.0666666666666667\n",
            "Sum of the page probabilities (second transition): 1.0000000000000002\n",
            "Page probabilities after second transition: [0.23333333 0.3        0.06666667 0.23333333 0.16666667]\n"
          ]
        }
      ],
      "source": [
        "# Compute the second transition of the Markov chain\n",
        "\n",
        "# 1. Compute and print the resulting page probabilities of the second transition\n",
        "p2 = transition(M, p1)\n",
        "print(\"Updated state vector p2:\", p2)  # Bullet 1\n",
        "\n",
        "# 2. Compute and print the Euclidean (L2) norm of the difference\n",
        "l2_norm_diff_2 = np.linalg.norm(p2 - p1)\n",
        "print(\"Euclidean (L2) norm of the difference (second transition):\", l2_norm_diff_2)  # Bullet 2\n",
        "\n",
        "# 3. Compute and print the sum of the page probabilities\n",
        "sum_p2 = np.sum(p2)\n",
        "print(\"Sum of the page probabilities (second transition):\", sum_p2)  # Bullet 3\n",
        "\n",
        "# 4. Display the page probabilities\n",
        "print(\"Page probabilities after second transition:\", p2)  # Bullet 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94bda45",
      "metadata": {
        "id": "b94bda45"
      },
      "source": [
        "> Note the difference between the Euclidean norms of the differences for the first and second transition calculations. Does the change in this difference from one step to the next indicate the algorithm is converging to the steady state probabilities?  \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52960810",
      "metadata": {
        "id": "52960810"
      },
      "source": [
        "> **Answer:** Yes, the change in the Euclidean norms of the differences between the first and second transition calculations indicates that the algorithm is converging to the steady state probabilities. The Euclidean norm of the difference decreased from approximately 0.1633 (first transition) to 0.0667 (second transition), showing that the changes in the page probabilities are becoming smaller with each transition. This decreasing trend suggests that the page probabilities are stabilizing and approaching their steady state values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cfc9527",
      "metadata": {
        "id": "6cfc9527"
      },
      "source": [
        "> **Exercise 04-5:** The question now is how does this simplified version of page rank converge with more iterations? To find out, do the following:   \n",
        "> 1. Create a function `pagerank1` having the following arguments, `the normalized transition matrix`, the `initial page probabilities` and a `convergence threshold value of 0.01`, which does the following:  \n",
        ">    - Initialize a euclidean distance norm variable to 1.0 and the resulting page probabilities to a vector of 0.0 values of length equal to the dimension of the transition matrix.   \n",
        ">    - Set a loop counter to 1.  \n",
        ">    - Use a 'while' loop with termination conditions the euclidean distance norm greater than the threshold value AND the loop counter less than 50.  Inside this loop do the following:  \n",
        ">      1. Update the page probabilities using the `transition` function you created.\n",
        ">      2. Compute the Euclidean norm of the difference between the previous and the updated page probabilities following the transition.   \n",
        ">      3. Print the value of the loop counter and the Euclidean norm of the difference.\n",
        ">      4. Copy the updated page probability vector into the input page probability vector.  \n",
        ">      5. Increment the loop counter by 1.\n",
        ">    - Return the page probabilities at convergence.  \n",
        "> 2. Execute your `pagerank1` function using the transition matrix and initial page probability vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b64228ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b64228ab",
        "outputId": "0a9d2be2-29cc-4f4e-94b7-5d22c6fb821c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Euclidean norm of the difference: 0.16329931618554527\n",
            "Iteration 2: Euclidean norm of the difference: 0.0666666666666667\n",
            "Iteration 3: Euclidean norm of the difference: 0.04082482904638634\n",
            "Iteration 4: Euclidean norm of the difference: 0.0285989726138528\n",
            "Iteration 5: Euclidean norm of the difference: 0.014829275350043464\n",
            "Iteration 6: Euclidean norm of the difference: 0.006843400694025186\n",
            "Final state probabilities: [0.25300926 0.28472222 0.07546296 0.22523148 0.16157407]\n"
          ]
        }
      ],
      "source": [
        "# 1. Define the PageRank function with a convergence threshold\n",
        "def pagerank1(M, in_probs, threshold=0.01):\n",
        "    # Initialize variables\n",
        "    euclidean_dist = 1.0\n",
        "    page_probabilities = np.array([0.0] * len(M))\n",
        "    i = 1\n",
        "\n",
        "    # Loop until convergence or max iterations\n",
        "    while euclidean_dist > threshold and i < 50:\n",
        "        # 1.1. Update the page probabilities\n",
        "        new_probs = transition(M, in_probs)\n",
        "\n",
        "        # 1.2. Compute the Euclidean norm of the difference\n",
        "        euclidean_dist = np.linalg.norm(new_probs - in_probs)\n",
        "\n",
        "        # 1.3. Print the value of the loop counter and the Euclidean norm of the difference\n",
        "        print(f\"Iteration {i}: Euclidean norm of the difference: {euclidean_dist}\")\n",
        "\n",
        "        # 1.4. Copy the updated page probability vector into the input page probability vector\n",
        "        in_probs = new_probs\n",
        "\n",
        "        # 1.5. Increment the loop counter\n",
        "        i += 1\n",
        "\n",
        "    # Return the page probabilities at convergence\n",
        "    return in_probs\n",
        "\n",
        "# Compute probabilities after a larger number of state transitions\n",
        "# Define the association matrix A\n",
        "A = np.array([\n",
        "    [0, 1, 1, 1, 0],  # Page 1\n",
        "    [1, 0, 0, 1, 1],  # Page 2\n",
        "    [0, 0, 0, 1, 0],  # Page 3\n",
        "    [0, 1, 0, 0, 1],  # Page 4\n",
        "    [1, 0, 1, 0, 0]   # Page 5\n",
        "])\n",
        "\n",
        "# Normalize the association matrix\n",
        "M = norm_association(A)\n",
        "\n",
        "# Define the initial state vector p with uniform initial probabilities\n",
        "p0 = np.full(M.shape[0], 1.0 / M.shape[0])\n",
        "\n",
        "# 2. Execute the pagerank1 function\n",
        "print('Final state probabilities: ' + str(pagerank1(M, p0)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0caeedfd",
      "metadata": {
        "id": "0caeedfd"
      },
      "source": [
        "> Provide short answers for the following questions:  \n",
        "> 1. Judged from the rate of decline of the Euclidean distances, does the algorithm appear to converge rapidly and why?\n",
        "> 2.  Does the rank order of the computed page probabilities make sense given the relative degree of the pages of the directed graph?\n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e95b5c5",
      "metadata": {
        "id": "0e95b5c5"
      },
      "source": [
        "> **Answers:**\n",
        "\n",
        "> 1. Yes, the algorithm appears to converge rapidly. The Euclidean distances between iterations decrease significantly with each step, indicating that the page probabilities are quickly stabilizing to their steady state values.\n",
        "\n",
        "> 2. Yes, the rank order of the computed page probabilities makes sense. Pages with higher in-degrees, which receive more links, have higher PageRank values, reflecting their greater importance or influence in the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee2af4a",
      "metadata": {
        "id": "aee2af4a"
      },
      "source": [
        "### Page Rank by Eigendecomposition\n",
        "\n",
        "Consider the relationship:     \n",
        "\n",
        "$$p_i = M p_{i-1}$$      \n",
        "\n",
        "At convergence $p_i = p_{i-1}$ which suggest an eigenvalue-eigenvector problem for some eigenvalue, $\\lambda$:    \n",
        "\n",
        "$$\\lambda p^* = M p^*$$  \n",
        "\n",
        "For the transition probability matrix with normalized columns, the largest eigenvalue has a magnitude 1.0. The eigenvector associated with this eigenvalue is the PageRank vector. To demonstrate this point, the code in the cell below does the following:     \n",
        "1. Compute the eigendecomposition using [numpy.linalg.eig](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html).     \n",
        "2. Print the magnitude of the eigenvalues.  \n",
        "3. Get the eigenvector associated with the first, largest, eigenvalue.     \n",
        "4. Normalize the eigenvector so the values sum to 1.0 and display the results.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2ed1809f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ed1809f",
        "outputId": "6e220cfd-fb05-41bd-c0ef-4acdce020576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magnitude of the eigenvalues: [1.00000000e+00 6.04992664e-01 5.86819216e-01 5.86819216e-01\n",
            " 3.95038058e-17]\n",
            "PageRank vector from eigendecomposition: [0.25373134 0.28358209 0.07462687 0.2238806  0.1641791 ]\n"
          ]
        }
      ],
      "source": [
        "# 1. Compute the eigendecomposition of the transition matrix M\n",
        "eigenvalues, eigenvectors = np.linalg.eig(M)\n",
        "\n",
        "# 2. Print the magnitude of the eigenvalues\n",
        "print(\"Magnitude of the eigenvalues:\", np.abs(eigenvalues))\n",
        "\n",
        "# 3. Get the eigenvector associated with the first, largest, eigenvalue\n",
        "principal_eigenvector = eigenvectors[:, np.argmax(np.abs(eigenvalues))]\n",
        "\n",
        "# 4. Normalize the eigenvector so the values sum to 1.0\n",
        "principal_eigenvector = principal_eigenvector / np.sum(principal_eigenvector)\n",
        "\n",
        "# Display the results\n",
        "print(\"PageRank vector from eigendecomposition:\", principal_eigenvector.real)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752fb6f7",
      "metadata": {
        "id": "752fb6f7"
      },
      "source": [
        "> **Exercise 04-06:** Do the PageRanks computed by the two methods agree to the precision expected with the iterative method?    \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce9b1eb",
      "metadata": {
        "id": "cce9b1eb"
      },
      "source": [
        "> **Answer:**\n",
        ">>\n",
        ">> Iterative Method Final State Probabilities:\n",
        ">>\n",
        ">> [0.25300926, 0.28472222, 0.07546296, 0.22523148, 0.16157407]\n",
        ">>\n",
        ">> Eigendecomposition PageRank Vector:\n",
        ">>\n",
        ">> [0.25373134, 0.28358209, 0.07462687, 0.2238806, 0.1641791]\n",
        ">\n",
        "> The values from both methods are very close to each other, with minor differences that are within the expected numerical precision. This small difference can be attributed to the iterative method's convergence criteria and floating-point arithmetic precision. Overall, both methods provide consistent and reliable PageRank values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1959889",
      "metadata": {
        "id": "a1959889"
      },
      "source": [
        "## A More Complicated Example   \n",
        "\n",
        "You will now work with a more complicated example The graph of 6 web pages, shown in Figure 2, is no longer complete. The out degree of page 6 is 0. A random surfer transitioning to page 6 will have no escape, a **spider trap**!\n",
        "\n",
        "<img src=\"../images/Web2.png\" alt=\"Drawing\" style=\"width:500px; height:500px\"/>\n",
        "<center>Figure 3: A small set of web pages with a dead end</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a4270c",
      "metadata": {
        "id": "d4a4270c"
      },
      "source": [
        "> **Exercise 04-7:** You will now create both the normalized transition matrix and the initial page probability vector for the graph of Figure 3. In this exercise you will . Do the following:  \n",
        "> 1. Create the association matrix and save it to a named variable, `A_deadend`. You will need this association marrix for later\n",
        "> 2. Normalize the association matrix using your `norm_association` function. Name your transition matrix `M_deadend`. Print the result.\n",
        "> 3. Create a vector containing the uniformly distributed initial probability values. Save and print the result.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "82db4316",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82db4316",
        "outputId": "9cc51764-3b22-4cec-a167-25876e8df2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized transition matrix M_deadend:\n",
            " [[0.         0.33333333 0.5        0.         0.         1.        ]\n",
            " [0.5        0.         0.         0.33333333 0.         0.        ]\n",
            " [0.         0.33333333 0.         0.33333333 0.5        0.        ]\n",
            " [0.5        0.33333333 0.         0.         0.5        0.        ]\n",
            " [0.         0.         0.5        0.33333333 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]]\n",
            "Initial state vector p0_deadend: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n"
          ]
        }
      ],
      "source": [
        "# 1. Create the association matrix for the graph in Figure 3\n",
        "A_deadend = np.array([\n",
        "    [0, 1, 1, 0, 0, 1],  # Page 1\n",
        "    [1, 0, 0, 1, 0, 0],  # Page 2\n",
        "    [0, 1, 0, 1, 1, 0],  # Page 3\n",
        "    [1, 1, 0, 0, 1, 0],  # Page 4\n",
        "    [0, 0, 1, 1, 0, 0],  # Page 5\n",
        "    [0, 0, 0, 0, 0, 0]   # Page 6 (dead end)\n",
        "])\n",
        "\n",
        "# 2. Normalize the association matrix using the norm_association function\n",
        "M_deadend = norm_association(A_deadend)\n",
        "print(\"Normalized transition matrix M_deadend:\\n\", M_deadend)\n",
        "\n",
        "# 3. Create a vector containing the uniformly distributed initial probability values\n",
        "p0_deadend = np.full(M_deadend.shape[0], 1.0 / M_deadend.shape[0])\n",
        "print(\"Initial state vector p0_deadend:\", p0_deadend)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210965cd",
      "metadata": {
        "id": "210965cd"
      },
      "source": [
        "> Examine your results. Are the 0 values for the transition probabilities of page 6 consistent with the graph of these pages? and why?      \n",
        "> **End of exercise.**     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e70927",
      "metadata": {
        "id": "05e70927"
      },
      "source": [
        "> **Answers:** Yes, the 0 values for the transition probabilities of page 6 are consistent with the graph of these pages. Page 6 is a dead end or spider trap, meaning it has no outbound links. In the association matrix, the row for Page 6 contains all 0s, indicating that there are no links from Page 6 to any other page. Consequently, in the normalized transition matrix, the column for Page 6 contains all 0s, indicating that no probability is distributed from Page 6 to any other page."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fab8ae8",
      "metadata": {
        "id": "3fab8ae8"
      },
      "source": [
        "### Apply Simple PageRank Algorithm\n",
        "\n",
        "> **Exercises 04-8:** What happens if you apply the simplified PageRank algorithm to the pages on a graph that is not complete, like the one shown in Figure 2? To find out, execute your `pagerank1` function with arguments `M_deadend`, `p_deadend` and `threshold=0.00001`. The smaller threshold value is to ensure convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dc2a2658",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc2a2658",
        "outputId": "2bdc4f12-812f-4f58-c680-6177780048b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Euclidean norm of the difference: 0.22906142364542562\n",
            "Iteration 2: Euclidean norm of the difference: 0.19289506162962652\n",
            "Iteration 3: Euclidean norm of the difference: 0.10088863659679724\n",
            "Iteration 4: Euclidean norm of the difference: 0.03412165422176333\n",
            "Iteration 5: Euclidean norm of the difference: 0.02590731238341437\n",
            "Iteration 6: Euclidean norm of the difference: 0.01671295239807624\n",
            "Iteration 7: Euclidean norm of the difference: 0.008603922335551545\n",
            "Iteration 8: Euclidean norm of the difference: 0.004451250160228181\n",
            "Iteration 9: Euclidean norm of the difference: 0.002525129392336522\n",
            "Iteration 10: Euclidean norm of the difference: 0.0015294848163096952\n",
            "Iteration 11: Euclidean norm of the difference: 0.0009792169050741875\n",
            "Iteration 12: Euclidean norm of the difference: 0.0006394541209994019\n",
            "Iteration 13: Euclidean norm of the difference: 0.00040849467529339994\n",
            "Iteration 14: Euclidean norm of the difference: 0.0002541321991486564\n",
            "Iteration 15: Euclidean norm of the difference: 0.0001564087085798745\n",
            "Iteration 16: Euclidean norm of the difference: 9.65445475097708e-05\n",
            "Iteration 17: Euclidean norm of the difference: 6.00675934039873e-05\n",
            "Iteration 18: Euclidean norm of the difference: 3.761384115338084e-05\n",
            "Iteration 19: Euclidean norm of the difference: 2.3604824036774115e-05\n",
            "Iteration 20: Euclidean norm of the difference: 1.4793491560482845e-05\n",
            "Iteration 21: Euclidean norm of the difference: 9.248971853728227e-06\n",
            "Final state probabilities: [0.17073152 0.164633   0.2317083  0.23780732 0.19511986 0.        ]\n"
          ]
        }
      ],
      "source": [
        "# Execute the pagerank1 function with M_deadend, p0_deadend and threshold=0.00001\n",
        "final_state_probabilities = pagerank1(M_deadend, p0_deadend, threshold=0.00001)\n",
        "print('Final state probabilities: ' + str(final_state_probabilities))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6380ffe4",
      "metadata": {
        "id": "6380ffe4"
      },
      "source": [
        "> Now, create and execute code to compute and display the eigenvalues and eigenvectors of the `M_deadend`. Then display the magnitudes of the eigenvalues, and the normalized values of the first eigenvalue (column 0).    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "878b096e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "878b096e",
        "outputId": "c76daed2-078b-47b7-c975-b07dacb013e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magnitude of the eigenvalues: [1.         0.21999481 0.44952418 0.44952418 0.62485456 0.        ]\n",
            "Normalized PageRank vector from eigendecomposition: [0.17073171 0.16463415 0.23170732 0.23780488 0.19512195 0.        ]\n"
          ]
        }
      ],
      "source": [
        "# Compute the eigendecomposition of the transition matrix M_deadend\n",
        "eigenvalues_deadend, eigenvectors_deadend = np.linalg.eig(M_deadend)\n",
        "\n",
        "# Print the magnitude of the eigenvalues\n",
        "print(\"Magnitude of the eigenvalues:\", np.abs(eigenvalues_deadend))\n",
        "\n",
        "# Get the eigenvector associated with the first, largest, eigenvalue\n",
        "principal_eigenvector_deadend = eigenvectors_deadend[:, np.argmax(np.abs(eigenvalues_deadend))]\n",
        "\n",
        "# Normalize the eigenvector so the values sum to 1.0\n",
        "principal_eigenvector_deadend = principal_eigenvector_deadend / np.sum(principal_eigenvector_deadend)\n",
        "\n",
        "# Display the results\n",
        "print(\"Normalized PageRank vector from eigendecomposition:\", principal_eigenvector_deadend.real)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e59ad36a",
      "metadata": {
        "id": "e59ad36a"
      },
      "source": [
        "> Answer the following questions:  \n",
        "> 1. Examine the page probabilities computed with the iterative methods. Do these PageRank values sum to 1.0 and why is this outcome a problem?       \n",
        "> 2. Examine the eigenvalues of the `M_deadend` matrix. What problem can you see with these eigenvalues?\n",
        "> 3. Notice the PageRank values have only 0 values. What does this tell you about the convergence of a random surfer on this graph?             \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sum of the final state probabilities:\", np.sum(final_state_probabilities))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGyGwGqvOVCb",
        "outputId": "4002b6c7-bd26-439d-bfe8-ac2204019399"
      },
      "id": "QGyGwGqvOVCb",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of the final state probabilities: 0.9999999999999993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a35139",
      "metadata": {
        "id": "87a35139"
      },
      "source": [
        "> **Answers:**\n",
        "\n",
        "> 1. These PageRank values might not sum exactly to 1.0, but the sum is very close: 0.9999999999999993, slightly off due to floating-point precision errors at times. PageRank values are supposed to represent a probability distribution, and probabilities must sum to 1. A discrepancy might indicate that the random surfer model isn't properly normalized\n",
        "\n",
        "> 2. The problem is that the largest eigenvalue is 1.0, but there is also an eigenvalue of 0. This zero eigenvalue indicates that there are dead ends or spider traps in the graph, where some pages (like Page 6) have no outbound links. This can cause issues with the convergence of the PageRank algorithm, as the probability \"leaks\" out of the system at these dead ends\n",
        "\n",
        "> 3. The PageRank vector from eigendecomposition shows a 0 value for Page 6. This indicates that a random surfer will get \"stuck\" at Page 6 if they reach it, because there are no outbound links from this page. This results in the probability accumulating at Page 6 and not being distributed back into the system. This behavior highlights the problem of dead ends in the graph and shows that the PageRank algorithm needs a mechanism (like teleportation or damping) to handle such cases and ensure proper convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a48973",
      "metadata": {
        "id": "b2a48973"
      },
      "source": [
        "### Damped PageRank Algorithm\n",
        "\n",
        "It is clear from the results of the foregoing exercise that the simple PageRank algorithm does not converge to a usable set of page probabilities when faced with graph that is not complete.Fortunately, there is a simple fix, add a damping term. You can think of the damping term as allowing a random surfer to make an arbitrary transition or jump with some small probability. These random jumps help the random surfer to better explore the graph and to escape from spider traps. The jump probabilities from states, $p_i$, are a function of the damping factor $d$:\n",
        "\n",
        "$$Jump\\ Probability = \\frac{(1-d)}{n}$$\n",
        "\n",
        "Where $n$ is the dimension of the transition probability matrix.\n",
        "\n",
        "The updated page probabilities, $p_i$, are then computed with the damped PageRank algorithm as:   \n",
        "\n",
        "$$p_{i} = d * M p_{i-1} + \\frac{(1-d)}{n}$$\n",
        "\n",
        "Where $M$ is the transition probability matrix and p are the initial page probability values.   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4cf21d2",
      "metadata": {
        "id": "f4cf21d2"
      },
      "source": [
        "> **Exercise 04-9:** To implement the PageRank algorithm with a damping factor do the following:  \n",
        "> 1. Create a `transition_damped` function with arguments, the transition probability matrix, the initial page probabilities, and the damping factor, $d=0.85$, which does the following:  \n",
        ">   - Compute the updated page probabili|ties by computing the inner (dot) product of the transition probability matrix with the page probabilities and then multiplying by the damping factor, `d`.    \n",
        ">   - Compute the jump probabilities vector of length the dimension of the transition matrix. Note: the jump probabilities are constant, so you can create code that only computes them once if you so choose.      \n",
        ">   - Return the sum of the damped page probabilities and the jump probabilities.  \n",
        "> 2. Create a `pagerank_damped` function. This function is identical to the `pagerank1` function you already created except that it uses the `transiton_damped` function in place of the `transition` function.  \n",
        "> 3. Call your `pagerank_damped` function using arguments of `M_deadend`, `p_deadend` and `threshold=0.0001` and display the final PageRank vector.\n",
        "> . Compute and display the sum of the values in the PageRank vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ca90f6a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca90f6a8",
        "outputId": "902235c6-4991-458f-8511-d3f97b37534e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized transition matrix M_deadend:\n",
            " [[0.         0.33333333 0.5        0.         0.         1.        ]\n",
            " [0.5        0.         0.         0.33333333 0.         0.        ]\n",
            " [0.         0.33333333 0.         0.33333333 0.5        0.        ]\n",
            " [0.5        0.33333333 0.         0.         0.5        0.        ]\n",
            " [0.         0.         0.5        0.33333333 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]]\n",
            "Initial state vector p0_deadend: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
            "Iteration 1: Euclidean norm of the difference: 0.19470221009861177\n",
            "Iteration 2: Euclidean norm of the difference: 0.13936668202740518\n",
            "Iteration 3: Euclidean norm of the difference: 0.06195823395000812\n",
            "Iteration 4: Euclidean norm of the difference: 0.0178117167640994\n",
            "Iteration 5: Euclidean norm of the difference: 0.011495212137118046\n",
            "Iteration 6: Euclidean norm of the difference: 0.006303281901598147\n",
            "Iteration 7: Euclidean norm of the difference: 0.0027582203701291322\n",
            "Iteration 8: Euclidean norm of the difference: 0.001212923493240815\n",
            "Iteration 9: Euclidean norm of the difference: 0.0005848627588229699\n",
            "Iteration 10: Euclidean norm of the difference: 0.00030111641215917174\n",
            "Iteration 11: Euclidean norm of the difference: 0.0001638653331708502\n",
            "Iteration 12: Euclidean norm of the difference: 9.095707777895543e-05\n",
            "Sum of the PageRank values = 0.9999999999999996\n",
            "Final state probabilities: [0.18476984 0.16808836 0.21385828 0.22781628 0.18046724 0.025     ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1. Create the transition_damped function:\n",
        "def transition_damped(transition_probs, probs, d=0.85):\n",
        "    '''Function to compute the probabilities resulting from a\n",
        "    single transition of a Markov process including a damping\n",
        "    factor to deal with dead ends'''\n",
        "    # Compute the updated page probabilities by computing the inner (dot) product\n",
        "    # of the transition probability matrix with the page probabilities and then\n",
        "    # multiplying by the damping factor, d.\n",
        "    damped_probs = d * np.dot(transition_probs, probs)\n",
        "\n",
        "    # Compute the jump probabilities vector of length the dimension of the transition matrix\n",
        "    n = len(transition_probs)\n",
        "    jump_probs = (1 - d) / n\n",
        "\n",
        "    # Return the sum of the damped page probabilities and the jump probabilities\n",
        "    return damped_probs + jump_probs\n",
        "\n",
        "# 2. Create the pagerank_damped function:\n",
        "def pagerank_damped(M, in_probs, d=0.85, threshold=0.01):\n",
        "    ## Function for the PageRank algorithm using the damped transition algorithm\n",
        "    euclidean_dist = 1.0\n",
        "    page_probabilities = np.array([0.0] * len(M))\n",
        "    i = 1\n",
        "\n",
        "    # Loop until convergence or max iterations\n",
        "    while euclidean_dist > threshold and i < 50:\n",
        "        new_probs = transition_damped(M, in_probs, d)\n",
        "        euclidean_dist = np.linalg.norm(new_probs - in_probs)\n",
        "        print(f\"Iteration {i}: Euclidean norm of the difference: {euclidean_dist}\")\n",
        "        in_probs = new_probs\n",
        "        i += 1\n",
        "\n",
        "    # Return the page probabilities at convergence\n",
        "    return in_probs\n",
        "\n",
        "# Compute probabilities after a larger number of state transitions\n",
        "# Define the association matrix A_deadend\n",
        "A_deadend = np.array([\n",
        "    [0, 1, 1, 0, 0, 1],  # Page 1\n",
        "    [1, 0, 0, 1, 0, 0],  # Page 2\n",
        "    [0, 1, 0, 1, 1, 0],  # Page 3\n",
        "    [1, 1, 0, 0, 1, 0],  # Page 4\n",
        "    [0, 0, 1, 1, 0, 0],  # Page 5\n",
        "    [0, 0, 0, 0, 0, 0]   # Page 6 (dead end)\n",
        "])\n",
        "\n",
        "# Normalize the association matrix\n",
        "M_deadend = norm_association(A_deadend)\n",
        "print(\"Normalized transition matrix M_deadend:\\n\", M_deadend)\n",
        "\n",
        "# Create a vector containing the uniformly distributed initial probability values\n",
        "p0_deadend = np.full(M_deadend.shape[0], 1.0 / M_deadend.shape[0])\n",
        "print(\"Initial state vector p0_deadend:\", p0_deadend)\n",
        "\n",
        "# 3. Execute the pagerank_damped function\n",
        "damped_rank = pagerank_damped(M_deadend, p0_deadend, threshold=0.0001)\n",
        "print(f\"Sum of the PageRank values = {np.sum(damped_rank)}\")\n",
        "print('Final state probabilities: ' + str(damped_rank))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4abd795",
      "metadata": {
        "id": "b4abd795"
      },
      "source": [
        "> Provide short answers to the following questions:   \n",
        "> 1. Examine the final page probabilities. Does the rank of these page probabilities make sense given the in degree of the pages of this graph?   \n",
        "> 2. Why is it reasonable that the sum of the PageRanks is $< 1.0$?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7927665",
      "metadata": {
        "id": "c7927665"
      },
      "source": [
        "> **Answers:**\n",
        "\n",
        "> 1. Yes, the rank of the final page probabilities makes sense given the in-degree of the pages. Pages with higher in-degrees tend to have higher PageRank values, as they receive more links from other pages. For example, Page 4 has the highest PageRank (0.22781628) and has multiple incoming links, which aligns with its higher in-degree\n",
        "\n",
        "> 2. The sum of the PageRanks being slightly less than 1.0 (0.9999999999999996) is due to floating-point precision errors inherent in numerical computations. In theory, the sum should be exactly 1.0, but due to the limitations of floating-point arithmetic, we observe a very small deviation from the expected value. This small discrepancy is generally acceptable in practical applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d6222f",
      "metadata": {
        "id": "e3d6222f"
      },
      "source": [
        "> Next you will examine the some properties of the damped matrix, $M$. You will do so by the following steps:    \n",
        "> 4. Create a Numpy array of $M$ including the damping, with a damping factor, $d = 0.85$. Display this matrix.  \n",
        "> 5. Compute and display the column sums of the damped matrix.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "871f9530",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "871f9530",
        "outputId": "d7be7637-b7e9-43e9-bc4b-511012065876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Damped matrix M with d=0.85:\n",
            " [[0.025      0.30833333 0.45       0.025      0.025      0.875     ]\n",
            " [0.45       0.025      0.025      0.30833333 0.025      0.025     ]\n",
            " [0.025      0.30833333 0.025      0.30833333 0.45       0.025     ]\n",
            " [0.45       0.30833333 0.025      0.025      0.45       0.025     ]\n",
            " [0.025      0.025      0.45       0.30833333 0.025      0.025     ]\n",
            " [0.025      0.025      0.025      0.025      0.025      0.025     ]]\n",
            "Column sums of the damped matrix: [1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "d = 0.85\n",
        "n = M_deadend.shape[0]\n",
        "damped_matrix = d * M_deadend + (1 - d) / n\n",
        "print(\"Damped matrix M with d=0.85:\\n\", damped_matrix)\n",
        "\n",
        "column_sums = np.sum(damped_matrix, axis=0)\n",
        "print(\"Column sums of the damped matrix:\", column_sums)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca1e229",
      "metadata": {
        "id": "3ca1e229"
      },
      "source": [
        "> Examine your results and asnwer these questions:   \n",
        "> 3. How does this array allow the random surfer to 'teleport' (or transition) from any page to any other page even when there is no directed edge?   \n",
        "> 4. Why are the column sums reasonable dispite the obvious devision from aximonatic probability theory?      \n",
        "> **End of exervise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed866eb",
      "metadata": {
        "id": "8ed866eb"
      },
      "source": [
        "> **Answers:**\n",
        "\n",
        "> 3. The damping factor introduces a uniform probability of transitioning to any page, including those without a direct link. This is achieved by adding a small jump probability, (1 - d) / n, to each element of the transition matrix. This ensures that every page has a non-zero probability of being visited, allowing the random surfer to 'teleport' to any page in the graph, regardless of the existence of a direct edge\n",
        "\n",
        "> 4. The column sums of the damped matrix are 1.0 because each column represents a probability distribution over all pages. The damping adjustment ensures that the total probability is conserved within the system, maintaining a valid probability distribution. Despite the addition of teleportation probabilities, the sum of probabilities from any given page remains 1.0, which is consistent with the requirements of a Markov process. This ensures the model's mathematical integrity while addressing practical issues like dead ends and spider traps in the graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a070f23d",
      "metadata": {
        "id": "a070f23d"
      },
      "source": [
        "## Hubs, Authorities, and the HITS Algorithm  \n",
        "\n",
        "The hubs and authorities model is an alternative to PageRank. Rather than using a single metric to rank the importance of web pages, the **HITS** algorithm iteratively updates the **hub score** and **authority score** for each of the pages.\n",
        "\n",
        "The HITS algorithm updates the authority and hub scores iteratively. The authority score is sum of the hubs linked to it. This is computed by the matrix product of the association matrix and hubness vector:\n",
        "$$𝑎= \\beta 𝐴 ℎ$$\n",
        "\n",
        "Hub score is sum of the authorities it links to. The hub score (hubness) is compute by the matrix produce of the authority scores and the transpose of the association matrix:\n",
        "$$ℎ= \\alpha 𝐴^𝑇 a$$\n",
        "\n",
        "The algorithm iterates between updates to $𝑎$ and $ℎ$ until convergence. To ensure convergence, must normalize $𝑎$ and $ℎ$ to have unit Euclidean norm at each iteration. Therefore, the choice of $\\alpha$ and $\\beta$ are can therefore be set to a value of 1.0, and effectively ignored.       "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d9ff8d",
      "metadata": {
        "id": "57d9ff8d"
      },
      "source": [
        "> **Exercise 04-10:** To understand the HITS algorithm you will now create and test code for this algorithm. Follow these steps:  \n",
        "> 1. Create a function called `HITS` with arguments of the association matrix, initial hub vector, initial authority vector, and the number of iterations of the algorithm to run. This function does the following inside a loop over the number of iterations:  \n",
        ">    1. Updates the authority vector using the association matrix and the hub vector as argument to the `transition` function.\n",
        ">    2. Normalizes the authority vector by using `numpy.divide` with arguments of the updated authority vector and its L2 norm, computed with [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).  \n",
        ">    3. Updates the hub vector using the transpose of the association matrix and the authority vector as argument to the `transition` function.\n",
        ">    4. Normalizes the hub vector by using `numpy.divide` with arguments of the updated hub vector and its L2 norm, computed with `numpy.linalg.norm`.  \n",
        "> 2. The function returns both the hub and authority vectors\n",
        "> 3. Initialize an initial hub and authority vector of length the dimension of the association matrix with uniformly distributed values of $\\frac{1.0}{dimension(association\\ matrix)}$.  \n",
        "> 4. Display the resulting hub and authority vectors.  \n",
        "> 5. Execute your function using the association matrix for the 6-page network and the initial hub and authority vectors as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b5426080",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5426080",
        "outputId": "6929ccd8-be76-4f78-9017-a8a02077ed75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial hub vector: [1. 1. 1. 1. 1. 1.]\n",
            "Initial authority vector: [1. 1. 1. 1. 1. 1.]\n",
            "The hub ranks = [0.33649127 0.60496025 0.27303459 0.47123038 0.44601115 0.1589491 ]\n",
            "The authority ranks = [0.40598191 0.31623732 0.59596894 0.54321619 0.29139291 0.        ]\n"
          ]
        }
      ],
      "source": [
        "def HITS(association, hub, authority, iters=100):\n",
        "    '''HITS algorithm implementation'''\n",
        "    for _ in range(iters):\n",
        "        # 1.1. Update the authority vector using the association matrix and the hub vector\n",
        "        authority = np.dot(association, hub)\n",
        "\n",
        "        # 1.2. Normalize the authority vector\n",
        "        authority = np.divide(authority, np.linalg.norm(authority))\n",
        "\n",
        "        # 1.3. Update the hub vector using the transpose of the association matrix and the authority vector\n",
        "        hub = np.dot(association.T, authority)\n",
        "\n",
        "        # 1.4. Normalize the hub vector\n",
        "        hub = np.divide(hub, np.linalg.norm(hub))\n",
        "\n",
        "    # 2. Return both the hub and authority vectors\n",
        "    return hub, authority\n",
        "\n",
        "# 3. Initialize an initial hub and authority vector of length the dimension of the association matrix\n",
        "# with uniformly distributed values of 1.0/dimension(association matrix)\n",
        "hub_start = np.ones(A_deadend.shape[0])\n",
        "auth_start = np.ones(A_deadend.shape[0])\n",
        "\n",
        "# 4. Display the resulting hub and authority vectors\n",
        "print(\"Initial hub vector:\", hub_start)\n",
        "print(\"Initial authority vector:\", auth_start)\n",
        "\n",
        "# 5. Execute the HITS function using the association matrix for the 6-page network\n",
        "# and the initial hub and authority vectors as arguments\n",
        "hubness, authority = HITS(A_deadend, hub_start, auth_start)\n",
        "print(f\"The hub ranks = {hubness}\")\n",
        "print(f\"The authority ranks = {authority}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95066c88",
      "metadata": {
        "id": "95066c88"
      },
      "source": [
        "> Examine your results and answer the following questions:\n",
        "> 1. Which three of the pages have the highest hub scores? Considering the graph of the pages, is this ordering consistent?  \n",
        "> 2. Notice the last value of the hub scores. Is this value expected given the graph of the pages?\n",
        "> 3. Which three of the pages have the highest authority. Given the in degree of the pages is this ranking consistent?  \n",
        "> 4. Compare the ranking of the pages based on authority that found with damped PageRank. Are these results consistent?\n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08eb85c2",
      "metadata": {
        "id": "08eb85c2"
      },
      "source": [
        "> **Answers:**\n",
        "\n",
        "> 1. The three pages with the highest hub scores are Page 2 (0.60496025), Page 4 (0.47123038), and Page 5 (0.44601115). Yes, this ordering is consistent with the graph because these pages link to several other pages with high authority, making them strong hubs\n",
        "\n",
        "> 2. The last value of the hub scores for Page 6 is 0.1589491. This value is expected given the graph because Page 6 is a dead end with no outbound links, making it a poor hub\n",
        "\n",
        "> 3. The three pages with the highest authority scores are Page 3 (0.59596894), Page 4 (0.54321619), and Page 1 (0.40598191). Yes, this ranking is consistent with the in-degree of the pages since these pages receive a higher number of inbound links, making them strong authorities\n",
        "\n",
        "> 4. The ranking based on authority scores is Page 3, Page 4, and Page 1. The ranking based on damped PageRank is Page 4, Page 3, and Page 1. These results are largely consistent. Both methods identify Page 3 and Page 4 as the top authorities, though the exact order differs slightly. This is expected because both algorithms rank pages based on their connectivity, but they use slightly different methods to calculate the importance of each page\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f99ee12d",
      "metadata": {
        "id": "f99ee12d"
      },
      "source": [
        "#### Copyright 2021, 2022, 2023, 2024 Stephen F Elston. All rights reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40de75c8",
      "metadata": {
        "id": "40de75c8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}